<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Confusion Matrix | virture.de - just a blog</title>
<meta name=keywords content="machine learning,confusion matrix,performance evaluation"><meta name=description content="Confusion Matrix"><meta name=author content="Michael"><link rel=canonical href=https://www.virture.de/posts/2023/confusion-matrix/><link crossorigin=anonymous href=/assets/css/stylesheet.1f28a8812024f3d082361c1abf7913baf83dd8b4bbf6c1463b9dbf1bb0c2d81d.css integrity="sha256-HyiogSAk89CCNhwav3kTuvg92LS79sFGO52/G7DC2B0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://www.virture.de/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.virture.de/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.virture.de/favicon-32x32.png><link rel=apple-touch-icon href=https://www.virture.de/apple-touch-icon.png><link rel=mask-icon href=https://www.virture.de/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Confusion Matrix"><meta property="og:description" content="Confusion Matrix"><meta property="og:type" content="article"><meta property="og:url" content="https://www.virture.de/posts/2023/confusion-matrix/"><meta property="og:image" content="https://www.virture.de/posts/2023/confusion-matrix/images/img13.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-03T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-03T00:00:00+00:00"><meta property="og:site_name" content="virture"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.virture.de/posts/2023/confusion-matrix/images/img13.jpg"><meta name=twitter:title content="Confusion Matrix"><meta name=twitter:description content="Confusion Matrix"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://www.virture.de/posts/"},{"@type":"ListItem","position":3,"name":"Confusion Matrix","item":"https://www.virture.de/posts/2023/confusion-matrix/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Confusion Matrix","name":"Confusion Matrix","description":"Confusion Matrix","keywords":["machine learning","confusion matrix","performance evaluation"],"articleBody":"Demystifying the Confusion Matrix: A Deep Dive into Evaluating Model Performance In the world of Machine Learning, understanding model performance is essential. One powerful tool for this purpose is the Confusion Matrix, a simple yet highly effective table layout for visualization and comprehension of your classifier’s performance.\nThe confusion matrix places the model’s predictions against the ground-truth data labels, creating an intuitive comparison grid. Each cell in this matrix represents a different aspect of the model’s performance, namely True Negatives (TN), False Negatives (FN), False Positives (FP) and True Positives (TP).\nUnderstanding the Confusion Matrix True Negatives (TN) The top-left cell in the matrix represents True Negatives. This indicates instances where the model predicted a negative result, and the actual label was indeed negative. In other words, these are the instances where the model correctly predicted a negative outcome.\nFalse Negatives (FN) The top-right cell represents False Negatives, indicating times when the model predicted a negative outcome, but the actual label was positive. This represents instances where the model should have predicted a positive outcome but did not.\nFalse Positives (FP) The bottom-left cell signifies False Positives. These are instances where the model predicted a positive outcome, but the actual label was negative. This denotes the occurrences where the model incorrectly predicted a positive outcome.\nTrue Positives (TP) Lastly, the bottom-right cell stands for True Positives, indicating times when the model correctly predicted a positive outcome.\nTo summarize, the key components of the confusion matrix are:\nTP = True positives: a positive label is correctly predicted. TN = True negatives: a negative label is correctly predicted. FP = False positives: a negative label is predicted as a positive. FN = False negatives: a positive label is predicted as a negative. Key Performance Metrics Derived from the Confusion Matrix Accuracy Accuracy is calculated as the number of correct predictions divided by the total number of predictions. It is a quick indicator of the overall performance of the model. However, accuracy may not be reliable when dealing with imbalanced datasets.\nMore generec accuracy fomularization:\naccuracy = number of correct predictions/total number of predictions Accuracy from pobability point of view:\naccuracy = (TP + TN) / number of samples Sensitivity/Recall (True Positive Rate) Sensitivity or Recall is a measure of the proportion of actual positive cases that were correctly identified by the model.\nrecall = sensitivity = TP / (TP + FN) Specificity Specificity denotes the fraction of actual negative cases that were correctly identified by the model.\nspecificity = TN / (TN + FP) Precision Precision represents the proportion of predicted positive cases that were correctly identified.\nprecision = TP / ( TP + FP) False Positive Rate (FPR) False Positive Rate denotes how often negative instances are incorrectly identified as positive.\nfalse_positive_rate = FP / ( FP + TN ) Key Note: The sum of specificity and false positive rate should always be 1!\nTrue Positive Rate (sensitivity) This metric illustrates how often “true” labels are correctly identified as “True”.\nFalse Positive Rate (False Alarm Rate) This illustrates how often “False” labels are incorrectly identified as “True”.\nWrapping Up The confusion matrix and its derived metrics provide a powerful and intuitive framework to evaluate the performance of classification models. By understanding each element and the relationships between them, we can gain valuable insights into our model’s strengths and weaknesses, and take steps to improve its performance.\n","wordCount":"565","inLanguage":"en","image":"https://www.virture.de/posts/2023/confusion-matrix/images/img13.jpg","datePublished":"2023-08-03T00:00:00Z","dateModified":"2023-08-03T00:00:00Z","author":{"@type":"Person","name":"Michael"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.virture.de/posts/2023/confusion-matrix/"},"publisher":{"@type":"Organization","name":"virture.de - just a blog","logo":{"@type":"ImageObject","url":"https://www.virture.de/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://www.virture.de accesskey=h title="home (Alt + H)"><img src=https://www.virture.de/images/robot.png alt aria-label=logo height=35>home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://www.virture.de/posts title=blog><span>blog</span></a></li><li><a href=https://www.virture.de/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://www.virture.de/categories/ title=categories><span>categories</span></a></li><li><a href=https://www.virture.de/tags/ title=tags><span>tags</span></a></li><li><a href=https://www.virture.de/archives title=archive><span>archive</span></a></li><li><a href=https://www.virture.de/about/ title=about><span>about</span></a></li><li><a href=https://www.virture.de/imprint/ title=imprint><span>imprint</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.virture.de>Home</a>&nbsp;»&nbsp;<a href=https://www.virture.de/posts/>Posts</a></div><h1 class=post-title>Confusion Matrix</h1><div class=post-description>Confusion Matrix</div></header><figure class=entry-cover><img loading=lazy src=https://www.virture.de/images/img13.jpg alt></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#understanding-the-confusion-matrix>Understanding the Confusion Matrix</a><ul><li><a href=#true-negatives-tn>True Negatives (TN)</a></li><li><a href=#false-negatives-fn>False Negatives (FN)</a></li><li><a href=#false-positives-fp>False Positives (FP)</a></li><li><a href=#true-positives-tp>True Positives (TP)</a></li></ul></li><li><a href=#key-performance-metrics-derived-from-the-confusion-matrix>Key Performance Metrics Derived from the Confusion Matrix</a><ul><li><a href=#accuracy>Accuracy</a></li><li><a href=#sensitivityrecall-true-positive-rate>Sensitivity/Recall (True Positive Rate)</a></li><li><a href=#specificity>Specificity</a></li><li><a href=#precision>Precision</a></li><li><a href=#false-positive-rate-fpr>False Positive Rate (FPR)</a></li><li><a href=#true-positive-rate-sensitivity>True Positive Rate (sensitivity)</a></li><li><a href=#false-positive-rate-false-alarm-rate>False Positive Rate (False Alarm Rate)</a></li></ul></li><li><a href=#wrapping-up>Wrapping Up</a></li></ul></nav></div></details></div><div class=post-content><h1 id=demystifying-the-confusion-matrix-a-deep-dive-into-evaluating-model-performance>Demystifying the Confusion Matrix: A Deep Dive into Evaluating Model Performance<a hidden class=anchor aria-hidden=true href=#demystifying-the-confusion-matrix-a-deep-dive-into-evaluating-model-performance>#</a></h1><p>In the world of Machine Learning, understanding model performance is essential. One powerful tool for this purpose is the <strong>Confusion Matrix</strong>, a simple yet highly effective table layout for visualization and comprehension of your classifier&rsquo;s performance.</p><p>The confusion matrix places the model&rsquo;s predictions against the ground-truth data labels, creating an intuitive comparison grid. Each cell in this matrix represents a different aspect of the model&rsquo;s performance, namely True Negatives (TN), False Negatives (FN), False Positives (FP) and True Positives (TP).</p><h2 id=understanding-the-confusion-matrix>Understanding the Confusion Matrix<a hidden class=anchor aria-hidden=true href=#understanding-the-confusion-matrix>#</a></h2><h3 id=true-negatives-tn>True Negatives (TN)<a hidden class=anchor aria-hidden=true href=#true-negatives-tn>#</a></h3><p>The top-left cell in the matrix represents True Negatives. This indicates instances where the model predicted a negative result, and the actual label was indeed negative. In other words, these are the instances where the model correctly predicted a negative outcome.</p><h3 id=false-negatives-fn>False Negatives (FN)<a hidden class=anchor aria-hidden=true href=#false-negatives-fn>#</a></h3><p>The top-right cell represents False Negatives, indicating times when the model predicted a negative outcome, but the actual label was positive. This represents instances where the model should have predicted a positive outcome but did not.</p><h3 id=false-positives-fp>False Positives (FP)<a hidden class=anchor aria-hidden=true href=#false-positives-fp>#</a></h3><p>The bottom-left cell signifies False Positives. These are instances where the model predicted a positive outcome, but the actual label was negative. This denotes the occurrences where the model incorrectly predicted a positive outcome.</p><h3 id=true-positives-tp>True Positives (TP)<a hidden class=anchor aria-hidden=true href=#true-positives-tp>#</a></h3><p>Lastly, the bottom-right cell stands for True Positives, indicating times when the model correctly predicted a positive outcome.</p><p>To summarize, the key components of the confusion matrix are:</p><ul><li>TP = True positives: a positive label is correctly predicted.</li><li>TN = True negatives: a negative label is correctly predicted.</li><li>FP = False positives: a negative label is predicted as a positive.</li><li>FN = False negatives: a positive label is predicted as a negative.</li></ul><h2 id=key-performance-metrics-derived-from-the-confusion-matrix>Key Performance Metrics Derived from the Confusion Matrix<a hidden class=anchor aria-hidden=true href=#key-performance-metrics-derived-from-the-confusion-matrix>#</a></h2><h3 id=accuracy>Accuracy<a hidden class=anchor aria-hidden=true href=#accuracy>#</a></h3><p>Accuracy is calculated as the number of correct predictions divided by the total number of predictions. It is a quick indicator of the overall performance of the model. However, accuracy may not be reliable when dealing with imbalanced datasets.</p><p>More generec accuracy fomularization:</p><pre tabindex=0><code>accuracy = number of correct predictions/total number of predictions
</code></pre><p>Accuracy from pobability point of view:</p><pre tabindex=0><code>accuracy = (TP + TN) / number of samples
</code></pre><h3 id=sensitivityrecall-true-positive-rate>Sensitivity/Recall (True Positive Rate)<a hidden class=anchor aria-hidden=true href=#sensitivityrecall-true-positive-rate>#</a></h3><p>Sensitivity or Recall is a measure of the proportion of actual positive cases that were correctly identified by the model.</p><pre tabindex=0><code>recall = sensitivity = TP / (TP + FN)
</code></pre><h3 id=specificity>Specificity<a hidden class=anchor aria-hidden=true href=#specificity>#</a></h3><p>Specificity denotes the fraction of actual negative cases that were correctly identified by the model.</p><pre tabindex=0><code>specificity = TN / (TN + FP)
</code></pre><h3 id=precision>Precision<a hidden class=anchor aria-hidden=true href=#precision>#</a></h3><p>Precision represents the proportion of predicted positive cases that were correctly identified.</p><pre tabindex=0><code>precision = TP / ( TP + FP)
</code></pre><h3 id=false-positive-rate-fpr>False Positive Rate (FPR)<a hidden class=anchor aria-hidden=true href=#false-positive-rate-fpr>#</a></h3><p>False Positive Rate denotes how often negative instances are incorrectly identified as positive.</p><pre tabindex=0><code>false_positive_rate = FP / ( FP + TN )
</code></pre><p><strong>Key Note</strong>: The sum of specificity and false positive rate should always be 1!</p><h3 id=true-positive-rate-sensitivity>True Positive Rate (sensitivity)<a hidden class=anchor aria-hidden=true href=#true-positive-rate-sensitivity>#</a></h3><p>This metric illustrates how often &ldquo;true&rdquo; labels are correctly identified as &ldquo;True&rdquo;.</p><h3 id=false-positive-rate-false-alarm-rate>False Positive Rate (False Alarm Rate)<a hidden class=anchor aria-hidden=true href=#false-positive-rate-false-alarm-rate>#</a></h3><p>This illustrates how often &ldquo;False&rdquo; labels are incorrectly identified as &ldquo;True&rdquo;.</p><h2 id=wrapping-up>Wrapping Up<a hidden class=anchor aria-hidden=true href=#wrapping-up>#</a></h2><p>The confusion matrix and its derived metrics provide a powerful and intuitive framework to evaluate the performance of classification models. By understanding each element and the relationships between them, we can gain valuable insights into our model&rsquo;s strengths and weaknesses, and take steps to improve its performance.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.virture.de/tags/machine-learning/>machine learning</a></li><li><a href=https://www.virture.de/tags/confusion-matrix/>confusion matrix</a></li><li><a href=https://www.virture.de/tags/performance-evaluation/>performance evaluation</a></li></ul><nav class=paginav><a class=prev href=https://www.virture.de/posts/2023/de-identification-techniques/><span class=title>« Prev</span><br><span>PII De-Identification techniques</span>
</a><a class=next href=https://www.virture.de/posts/2023/collaborative-and-content-based-filtering/><span class=title>Next »</span><br><span>Collaborative and Content-based Filtering</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://www.virture.de>virture.de - just a blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>