<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>L1 and L2 Regularization | virture.de - just a blog</title><meta name=keywords content="machine learning,normalization,regularization"><meta name=description content="L1 versus L2 Regularization"><meta name=author content="Michael"><link rel=canonical href=https://www.virture.de/posts/2023/l1-and-l2-regularization/><link crossorigin=anonymous href=/assets/css/stylesheet.1f28a8812024f3d082361c1abf7913baf83dd8b4bbf6c1463b9dbf1bb0c2d81d.css integrity="sha256-HyiogSAk89CCNhwav3kTuvg92LS79sFGO52/G7DC2B0=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.virture.de/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.virture.de/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.virture.de/favicon-32x32.png><link rel=apple-touch-icon href=https://www.virture.de/apple-touch-icon.png><link rel=mask-icon href=https://www.virture.de/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="L1 and L2 Regularization"><meta property="og:description" content="L1 versus L2 Regularization"><meta property="og:type" content="article"><meta property="og:url" content="https://www.virture.de/posts/2023/l1-and-l2-regularization/"><meta property="og:image" content="https://www.virture.de/posts/2023/l1-and-l2-regularization/images/img16.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-06T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-06T00:00:00+00:00"><meta property="og:site_name" content="virture"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.virture.de/posts/2023/l1-and-l2-regularization/images/img16.png"><meta name=twitter:title content="L1 and L2 Regularization"><meta name=twitter:description content="L1 versus L2 Regularization"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://www.virture.de/posts/"},{"@type":"ListItem","position":3,"name":"L1 and L2 Regularization","item":"https://www.virture.de/posts/2023/l1-and-l2-regularization/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"L1 and L2 Regularization","name":"L1 and L2 Regularization","description":"L1 versus L2 Regularization","keywords":["machine learning","normalization","regularization"],"articleBody":"L1 vs. L2 Regularization: A Comparison in Machine Learning In the realm of machine learning, regularization techniques play a crucial role in controlling model complexity and preventing overfitting. Two popular regularization methods are L1 and L2 regularization, each with its distinct characteristics and impact on model weights.\nL2 Regularization L2 regularization, also known as Ridge regularization, penalizes the sum of squared weights in a model. Mathematically, it adds the square of each weight to the loss function, discouraging large weight values.\nL2 penalizes weights by their square value. The derivative of L2 is directly proportional to the weight, resulting in a gradual reduction in weight values over time. Although L2 discourages large weights, it does not drive weights to absolute zero, ensuring that all features contribute to the model.\nL1 Regularization On the other hand, L1 regularization, also called Lasso regularization, penalizes the sum of the absolute values of the weights. Unlike L2, L1 regularization has a unique characteristic related to absolute values, which makes it an efficient choice for feature selection.\nL1 penalizes weights by their absolute value, which effectively enforces sparsity in the model. The derivative of L1 is a constant (k) that remains independent of weight values, leading to sudden weight updates during optimization.\nDue to the absolute value, L1 regularization introduces a discontinuity at zero, causing certain weight updates to be zeroed out. L1 regularization is particularly useful for feature selection as it can drive some weights to exact zero, effectively excluding irrelevant features from the model.\nChoosing the Right Regularization Deciding between L1 and L2 regularization depends on the specific characteristics of the dataset and the problem at hand. L2 regularization is generally well-suited for cases where all features are expected to contribute to the model, preventing any single feature from dominating the prediction process. On the other hand, L1 regularization shines in situations where feature sparsity is desired, allowing for more interpretable models and efficient representation of wide datasets.\nIn conclusion, understanding the differences between L1 and L2 regularization is essential for effectively applying regularization techniques in machine learning models. By carefully selecting the appropriate regularization method, data scientists and machine learning practitioners can achieve better generalization and more meaningful insights from their models.\nRemember, the choice between L1 and L2 regularization is just one aspect of model regularization. In practice, it’s common to explore a combination of regularization techniques and tune their hyperparameters to achieve the best model performance for a given task.\n","wordCount":"410","inLanguage":"en","image":"https://www.virture.de/posts/2023/l1-and-l2-regularization/images/img16.png","datePublished":"2023-08-06T00:00:00Z","dateModified":"2023-08-06T00:00:00Z","author":{"@type":"Person","name":"Michael"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.virture.de/posts/2023/l1-and-l2-regularization/"},"publisher":{"@type":"Organization","name":"virture.de - just a blog","logo":{"@type":"ImageObject","url":"https://www.virture.de/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://www.virture.de accesskey=h title="home (Alt + H)"><img src=https://www.virture.de/images/robot.png alt aria-label=logo height=35>home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://www.virture.de/posts title=blog><span>blog</span></a></li><li><a href=https://www.virture.de/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://www.virture.de/categories/ title=categories><span>categories</span></a></li><li><a href=https://www.virture.de/tags/ title=tags><span>tags</span></a></li><li><a href=https://www.virture.de/archives title=archive><span>archive</span></a></li><li><a href=https://www.virture.de/about/ title=about><span>about</span></a></li><li><a href=https://www.virture.de/imprint/ title=imprint><span>imprint</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://www.virture.de>Home</a>&nbsp;»&nbsp;<a href=https://www.virture.de/posts/>Posts</a></div><h1 class=post-title>L1 and L2 Regularization</h1><div class=post-description>L1 versus L2 Regularization</div><div class=post-meta><span title='2023-08-06 00:00:00 +0000 UTC'>August 6, 2023</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;410 words&nbsp;·&nbsp;Michael</div></header><figure class=entry-cover><img loading=lazy src=https://www.virture.de/images/img16.png alt></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#l1-vs-l2-regularization-a-comparison-in-machine-learning>L1 vs. L2 Regularization: A Comparison in Machine Learning</a><ul><li><a href=#l2-regularization>L2 Regularization</a></li><li><a href=#l1-regularization>L1 Regularization</a></li><li><a href=#choosing-the-right-regularization>Choosing the Right Regularization</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=l1-vs-l2-regularization-a-comparison-in-machine-learning>L1 vs. L2 Regularization: A Comparison in Machine Learning<a hidden class=anchor aria-hidden=true href=#l1-vs-l2-regularization-a-comparison-in-machine-learning>#</a></h2><p>In the realm of machine learning, regularization techniques play a crucial role in controlling model complexity and preventing overfitting. Two popular regularization methods are L1 and L2 regularization, each with its distinct characteristics and impact on model weights.</p><h3 id=l2-regularization>L2 Regularization<a hidden class=anchor aria-hidden=true href=#l2-regularization>#</a></h3><ul><li><p>L2 regularization, also known as Ridge regularization, penalizes the sum of squared weights in a model. Mathematically, it adds the square of each weight to the loss function, discouraging large weight values.</p></li><li><p>L2 penalizes weights by their square value.
The derivative of L2 is directly proportional to the weight, resulting in a gradual reduction in weight values over time.
Although L2 discourages large weights, it does not drive weights to absolute zero, ensuring that all features contribute to the model.</p></li></ul><h3 id=l1-regularization>L1 Regularization<a hidden class=anchor aria-hidden=true href=#l1-regularization>#</a></h3><ul><li><p>On the other hand, L1 regularization, also called Lasso regularization, penalizes the sum of the absolute values of the weights. Unlike L2, L1 regularization has a unique characteristic related to absolute values, which makes it an efficient choice for feature selection.</p></li><li><p>L1 penalizes weights by their absolute value, which effectively enforces sparsity in the model.
The derivative of L1 is a constant (k) that remains independent of weight values, leading to sudden weight updates during optimization.</p></li></ul><p>Due to the absolute value, L1 regularization introduces a discontinuity at zero, causing certain weight updates to be zeroed out.
L1 regularization is particularly useful for feature selection as it can drive some weights to exact zero, effectively excluding irrelevant features from the model.</p><h3 id=choosing-the-right-regularization>Choosing the Right Regularization<a hidden class=anchor aria-hidden=true href=#choosing-the-right-regularization>#</a></h3><p>Deciding between L1 and L2 regularization depends on the specific characteristics of the dataset and the problem at hand. L2 regularization is generally well-suited for cases where all features are expected to contribute to the model, preventing any single feature from dominating the prediction process. On the other hand, L1 regularization shines in situations where feature sparsity is desired, allowing for more interpretable models and efficient representation of wide datasets.</p><p>In conclusion, understanding the differences between L1 and L2 regularization is essential for effectively applying regularization techniques in machine learning models. By carefully selecting the appropriate regularization method, data scientists and machine learning practitioners can achieve better generalization and more meaningful insights from their models.</p><p>Remember, the choice between L1 and L2 regularization is just one aspect of model regularization. In practice, it&rsquo;s common to explore a combination of regularization techniques and tune their hyperparameters to achieve the best model performance for a given task.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.virture.de/tags/machine-learning/>machine learning</a></li><li><a href=https://www.virture.de/tags/normalization/>normalization</a></li><li><a href=https://www.virture.de/tags/regularization/>regularization</a></li></ul><nav class=paginav><a class=prev href=https://www.virture.de/posts/2023/rnn-cmm/><span class=title>« Prev</span><br><span>Use Case for Recurrent Neural Network and Convolutional Neural Network</span></a>
<a class=next href=https://www.virture.de/posts/2023/hyperparameter-tuning/><span class=title>Next »</span><br><span>Hyperparameter Tuning</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://www.virture.de>virture.de - just a blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>