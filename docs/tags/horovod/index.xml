<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Horovod on virture.de - just a blog</title>
    <link>https://www.virture.de/tags/horovod/</link>
    <description>Recent content in Horovod on virture.de - just a blog</description>
    <image>
      <title>virture.de - just a blog</title>
      <url>https://www.virture.de/images/mh.jpg</url>
      <link>https://www.virture.de/images/mh.jpg</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 09 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://www.virture.de/tags/horovod/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Horovod Distributed Training Framework</title>
      <link>https://www.virture.de/posts/2023/horovord-distribution/</link>
      <pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.virture.de/posts/2023/horovord-distribution/</guid>
      <description>Horovod Distributed Training Framework Horovod is a distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. It is designed to make distributed training easy and efficient. Horovod uses a ring-based communication pattern to efficiently distribute data across multiple GPUs or machines. This can significantly improve the training speed of deep learning models.
Horovod is also designed to be easy to use. It can be used with existing TensorFlow, Keras, PyTorch, and Apache MXNet code with minimal changes.</description>
    </item>
    
  </channel>
</rss>
