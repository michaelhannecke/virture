[{"content":"Unlocking Fairness in AI: Navigating the Challenges of Bias in Machine Learning Introduction: In the competitive landscape of modern business, AI and Machine Learning (ML) are no longer optional; they are essential tools driving innovation, efficiency, and growth. However, like any powerful tool, they come with challenges that need to be expertly managed. One of these challenges is the risk of bias in AI models, a subtle but pervasive issue that can undermine not only the performance of an AI system but the reputation and ethical standing of a business.\nBias in ML models can manifest in many forms, ranging from data collection to algorithm selection, and even the way results are interpreted. For businesses seeking to leverage AI, understanding, and mitigating these biases is not merely an ethical responsibility; it\u0026rsquo;s a strategic imperative.\nMachine learning models and generative AI applications can be affected by various forms of bias, stemming from both the data they are trained on and the way the models are designed and used. Here\u0026rsquo;s an overview:\nSorting biases by their risk or potential damage can be subjective and context dependent. What might be most damaging in one situation may not be in another. However, I can attempt to provide a general ordering based on a common understanding of potential impacts:\nSocietal and Ethical Bias: This can lead to serious consequences by reinforcing existing inequalities and biases in society. It can result in systemic discrimination, impacting large sections of the population.\nModels might be biased due to the broader societal and ethical beliefs, norms, or regulations at the time of their creation.\nExample A predictive policing algorithm that is trained on historical crime data may reinforce societal biases, such as racial or socio-economic biases present in the data. By allocating more police resources to areas with historically higher crime rates, the system may disproportionately target minority or disadvantaged communities, reflecting broader societal and ethical biases.\nCounter Measurement Consider societal and ethical implications during design and deployment, engage with diverse stakeholders, and possibly utilize third-party audits to ensure fairness and alignment with social values.\nFeedback Loop Bias This type of bias can perpetuate and amplify existing biases in a self-reinforcing manner, leading to the entrenchment of biased decisions and potentially widening societal inequalities. In interactive systems, biased predictions can lead to biased actions, which then lead to more biased data, and so on, in a feedback loop.\nExample An algorithm used to filter job applications may initially have a slight bias against candidates from a particular background. As the algorithm continues to influence hiring decisions, fewer individuals from that background are hired. This leads to even more biased training data over time, reinforcing and amplifying the initial bias in a continuous feedback loop.\nCounter measurement Regularly update the model with fresh, unbiased data and monitor its behavior over time to detect and correct any emerging biases.\nPrejudice Bias As part of data bias, this can result in highly unfair and discriminatory models that reflect and perpetuate harmful stereotypes. It can affect individuals\u0026rsquo; rights and opportunities. When data is collected and labeled based on prejudiced human beliefs, it might incorporate human biases such as racism or sexism.\nExample If a facial recognition system is trained primarily on images of light-skinned individuals and lacks diversity in skin tones, it may perform poorly on recognizing individuals with darker skin tones. This can result in racial bias, where the model\u0026rsquo;s performance is uneven across different racial groups, reflecting a prejudiced data collection process.\nCounter Measurement Prejudice Bias: Actively seek out and include diverse data points and consider ethical implications during data collection and preprocessing to avoid encoding prejudiced human beliefs.\nHistorical Bias This also can perpetuate societal inequalities if historical human biases are reflected in AI decisions. It can lead to the continuation of past injustices.\nIf the data includes societal or cultural biases, the model may learn these biases as well.\nExample If you\u0026rsquo;re building a hiring algorithm using data from a company that has historically favored hiring men for certain roles, the model may learn this bias and favor male candidates in its predictions, even if gender is not explicitly used as a feature.\nCounter measurement Historical Bias: Understand the historical context and societal biases that might be reflected in the data. If possible, adjust or re-weight the data to minimize these biases, and include features that capture potentially confounding variables. Deployment Bias Applying a model to a population or context different from its training data might cause substantial harm, especially if it leads to systematic misjudgment in critical areas like healthcare, finance, or legal decisions. Once in production, the model might be applied in contexts or to populations that it was not trained on, leading to biases in its predictions.\nExample A credit scoring model trained on data from a particular country or demographic group might be deployed in a different region with a different economic context. The model may perform poorly or unfairly in the new context because it was not trained on data representative of the population it\u0026rsquo;s being applied to.\nCounter Measurement Validate the model in the context where it will be deployed, using data that is representative of the target population or environment, and be prepared to make adjustments as needed.\nGroup Bias This can lead to discrimination against certain demographic groups, which can be damaging both to individuals within those groups and to social cohesion more broadly.\nSome models may have biased performance towards different demographic groups, leading to unfair treatments or outcomes.\nExample A facial recognition system might be trained on a dataset primarily composed of people from one ethnic group. As a result, the system may perform well for that group but poorly for others, leading to biased performance across different demographic groups.\nCounter measurement Implement fairness-aware machine learning techniques that account for demographic or group-based differences and validate the model across different groups to ensure equitable performance.\nSampling Bias This type of data bias can lead to models that systematically misrepresent reality, which might lead to flawed decisions with varying degrees of risk depending on the application.\nIf the training data isn\u0026rsquo;t representative of the population, the model will have a biased understanding.\nExample Suppose you\u0026rsquo;re building a voice recognition system and collect voice samples only from young adults in the United States. The model might perform poorly on accents from other regions or age groups because those voices were not represented in the training data.\nCounter Measurement Ensure that the data collection process represents the population accurately by including diverse and representative samples. Stratified sampling can help in achieving this balance.\nMeasurement Bias If measurements are consistently erroneous, the subsequent decisions can be highly flawed, leading to misleading insights and possibly risky actions.\nErrors in measuring variables or attributes can introduce biases into the data and, consequently, the trained model\nExample You are creating a health prediction model using data collected from various sensors, like heart rate monitors. If one type of sensor is consistently inaccurate or calibrated differently from others, the data will have systematic errors. The model trained on this data might then have biases in predictions, favoring or penalizing readings from that specific sensor type.\nCounter Measurement Ensure that the measurement tools and processes are validated, calibrated, and standardized. Test them across various scenarios to verify their accuracy.\nConfirmation Bias If a model is continuously fine-tuned on predictions that it makes, it may amplify its own biases.\nExample Imagine training a stock market prediction model. If you continually fine-tune the model based on its own predictions, rather than independent, fresh data, the model might become overly confident in its predictions and reinforce its own errors or biases. This creates a situation where the model is essentially confirming its own predictions, leading to a possible decrease in predictive accuracy.\nCounter Measurement Encourage practices that foster unbiased evaluation, such as cross-validation with diverse datasets, and promoting a culture of challenging assumptions within the team.\nLabeling Bias Incorrect or inconsistent labels can lead to a model misunderstanding the relationships in the data, possibly leading to wrong predictions with varying levels of damage depending on the context.\nMislabeling or inconsistent labeling of the data might skew the model\u0026rsquo;s understanding.\nExample Consider a sentiment analysis model trained on data where reviewers labeled sarcasm as positive sentiment. If the labeling doesn\u0026rsquo;t capture the true sentiment behind sarcastic comments, the model might interpret all sarcastic remarks as positive, leading to incorrect predictions.\nCounter Measurement Implement rigorous quality control and standardization processes for labeling, and consider having multiple human annotators provide labels to reduce inconsistencies.\nClass Imbalance While it can lead to poor performance on minority classes, its damage might be more confined compared to more pervasive biases, depending on the application.\nIf some classes are underrepresented in the training data, the model may perform poorly on those classes.\nExample In a medical diagnosis model, if a rare disease is underrepresented in the training data (e.g., only 5 out of 1000 examples), the model may learn to mostly predict the more common classes, leading to poor performance in detecting that rare disease.\nCounter Measurement Utilize techniques like oversampling minority classes, under-sampling majority classes, or employing specialized algorithms designed to handle class imbalance.\nSelection Bias (Algorithm Bias) Although it can skew the model towards particular features, the degree of risk might be more context-specific and potentially less harmful compared to biases that directly perpetuate discrimination.\nIf certain features are given undue importance during feature selection, it might bias the predictions.\nExample Suppose you are developing a recommendation system for movies. If you only select features related to a specific genre, such as action movies (e.g., explosions, fight scenes), the model might become biased towards recommending action movies and ignore other potentially relevant movies in different genres. This reflects a bias in the feature selection process, as you have given undue importance to certain characteristics.\nCounter Measurement Utilize feature selection methods that rely on objective criteria and validate the model with independent data sets to ensure that the selection is not inadvertently biased.\nObserver Bias This can distort the understanding of a model but might be considered less harmful as it primarily affects interpretation rather than the model\u0026rsquo;s functioning itself. The influence of the researchers\u0026rsquo; expectations on the data collection, labeling, or interpretation can introduce bias.\nExample Consider a project where human experts are labeling images for a facial expression recognition system. If the labelers have preconceived notions about what certain expressions look like on people from different cultural backgrounds, their labels might reflect these biases. The model trained on this data will then learn these observer biases, potentially leading to uneven performance across different cultural groups.\nCounter Measurement Encourage multiple interpretations from different stakeholders or team members, fostering a culture that values diverse perspectives and critical evaluation.\nConfirmation Bias in Interpretation This bias affects how results are interpreted but may not affect the model\u0026rsquo;s operation directly, limiting its potential damage.\nHumans might interpret the results of a model in ways that confirm their existing beliefs or expectations, leading to biased conclusions.\nExample A data scientist has a hypothesis that a certain feature, such as user activity level, is the most important factor in predicting user engagement. When interpreting the results, they may focus on evidence that confirms this belief, overlooking other factors that could be equally or more important. This selective attention to the results that confirm their pre-existing belief leads to a biased interpretation of the model\u0026rsquo;s behavior.\nCounter Measurement Actively seek out and consider evidence that contradicts initial interpretations or hypotheses, and involve different stakeholders with varying perspectives in the interpretation process.\nExperimenter Bias While this can distort the research process, its impact might be limited to specific studies or experiments rather than having broad societal implications.\nThis might happen when researchers unintentionally influence the results of a study to reach a predetermined conclusion.\nExample A researcher developing an algorithm for a specific medical diagnosis might unconsciously prefer a particular modeling technique. This preference might lead them to design experiments in a way that makes this technique appear more effective than others, even if that\u0026rsquo;s not objectively the case.\nCounter Measurement Employ rigorous experimental design principles, possibly utilizing double-blind procedures or other methods to minimize the influence of individual researchers\u0026rsquo; preferences or expectations.\nEvaluation Bias Though it can lead to incorrect conclusions about a model\u0026rsquo;s performance, it may not directly cause harm to individuals or society but rather affects the understanding of the model\u0026rsquo;s capability.\nIf the evaluation metrics or test data are biased, they might lead to misleading conclusions about the model\u0026rsquo;s performance.\nExample If a speech recognition model is evaluated only on a specific accent or dialect, the evaluation might indicate high performance. However, this evaluation is biased if the model is intended to be used by speakers of various accents and dialects, as it doesn\u0026rsquo;t truly reflect the model\u0026rsquo;s ability to handle diverse inputs.\nCounter Measurement Use diverse and representative datasets for evaluation and consider employing multiple evaluation metrics that capture different aspects of performance.\nAnchoring Bias: Typically considered a cognitive bias, its effects may be relatively minor in comparison to others, affecting decision-making processes rather than leading to systematic errors or discrimination.\nRelying too heavily on the first piece of information encountered when making decisions during model development.\nExample A team is developing a recommendation algorithm and initially focuses on user click behavior as the primary feature. Throughout the development process, they may become anchored to this feature, giving it undue importance and overlooking other potentially valuable information. Even when presented with new data or insights, they may have difficulty adjusting their approach due to this initial anchoring.\nCounter Measurement Foster a culture that encourages reevaluation and reconsideration of initial decisions or hypotheses, and promote collaboration and diverse perspectives to avoid undue focus on early assumptions.\nBy employing these countermeasures, organizations can mitigate biases at various stages of the machine learning process, improving the fairness, robustness, and generalizability of their models.\nSummary Addressing biases in AI and ML is a complex, multifaceted task that demands a concerted effort across various stages of model development, deployment, and monitoring. In the rapidly evolving world of business, where AI-driven decisions are becoming more commonplace, understanding the nature of these biases, and implementing strategies to mitigate them is crucial.\nThe countermeasures include ensuring diversity in data, validating models in real-world contexts, employing fairness-aware machine learning techniques, and fostering a culture of critical evaluation and continuous learning. By doing so, businesses can build more robust, fair, and effective AI systems, aligning technology with core values and strategic goals. Navigating the challenges of bias in machine learning is not just about avoiding pitfalls; it\u0026rsquo;s about unlocking the full potential of AI. In a business world driven by data and powered by intelligent algorithms, a proactive approach to bias can lead to more informed decisions, enhanced trust with customers, and a competitive edge in the marketplace.\n","permalink":"https://www.virture.de/posts/2023/aa-bias-in-ml/","summary":"Unlocking Fairness in AI: Navigating the Challenges of Bias in Machine Learning Introduction: In the competitive landscape of modern business, AI and Machine Learning (ML) are no longer optional; they are essential tools driving innovation, efficiency, and growth. However, like any powerful tool, they come with challenges that need to be expertly managed. One of these challenges is the risk of bias in AI models, a subtle but pervasive issue that can undermine not only the performance of an AI system but the reputation and ethical standing of a business.","title":"Bias in Machine Learning"},{"content":"Hyperparameter Tuning: Best Practices and Insights Hyperparameter tuning is a critical step in training your machine learning model, as it directly influences the model\u0026rsquo;s performance. This article discusses some key insights and practices to enhance the effectiveness of hyperparameter tuning.\nTraining Loss and its Implications Convergence of Training Loss: Ideally, the training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. This is a sign that the model is learning and adapting to the patterns in the training data. If the training loss does not converge, consider increasing the number of training epochs. Slow Decrease in Training Loss: If the training loss decreases too slowly during the training process, it might be a sign that the learning rate is too low. In such cases, consider increasing the learning rate. Be cautious, as setting the learning rate too high may prevent the training loss from converging. Variation in Training Loss: If the training loss jumps around or varies wildly, it could imply that the learning rate is too high. Decrease the learning rate to achieve a more stable decrease in training loss. Finding the Right Learning Rate and Batch Size Balancing Learning Rate, Epochs, and Batch Size: A common practice in hyperparameter tuning is to lower the learning rate while increasing the number of epochs or the batch size. This often results in better model performance. Firstly, try using large batch size values, and then decrease the batch size until you observe degradation in the model\u0026rsquo;s performance. Small Batch Sizes: Be aware that setting the batch size to a very small number can cause instability in the training process. It\u0026rsquo;s typically better to start with larger batch sizes and gradually decrease them until you observe a decrease in performance. Large Datasets: For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you\u0026rsquo;ll need to reduce the batch size to enable a batch to fit into memory. Final Thoughts Hyperparameter tuning is more of an art than a science. While these guidelines provide a good starting point, the best configuration often depends on the specifics of your dataset and model. It\u0026rsquo;s important to experiment with different combinations and tune the hyperparameters based on the feedback received from the model\u0026rsquo;s performance. Remember, patience and systematic exploration often yield the best results in hyperparameter tuning.\n","permalink":"https://www.virture.de/posts/2023/hyperparameter-tuning/","summary":"Hyperparameter Tuning: Best Practices and Insights Hyperparameter tuning is a critical step in training your machine learning model, as it directly influences the model\u0026rsquo;s performance. This article discusses some key insights and practices to enhance the effectiveness of hyperparameter tuning.\nTraining Loss and its Implications Convergence of Training Loss: Ideally, the training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.","title":"Hyperparameter Tuning"},{"content":"Types of (PII) de-identification techniques Choosing the de-identification transformation you want to use depends on the kind of data you want to de-identify and for what purpose you\u0026rsquo;re de-identifying the data. The de-identification techniques that Sensitive Data Protection supports fall into the following general categories:\nRedaction: Deletes all or part of a detected sensitive value.\nReplacement: Replaces a detected sensitive value with a specified surrogate value.\nMasking: Replaces a number of characters of a sensitive value with a specified surrogate character, such as a hash (#) or asterisk (*).\nCrypto-based tokenization: Encrypts the original sensitive data value using a cryptographic key. Sensitive Data Protection supports several types of tokenization, including transformations that can be reversed, or \u0026ldquo;re-identified.\u0026rdquo;\nBucketing: \u0026ldquo;Generalizes\u0026rdquo; a sensitive value by replacing it with a range of values. (For example, replacing a specific age with an age range, or temperatures with ranges corresponding to \u0026ldquo;Hot,\u0026rdquo; \u0026ldquo;Medium,\u0026rdquo; and \u0026ldquo;Cold.\u0026rdquo;)\nDate shifting: Shifts sensitive date values by a random amount of time.\nTime extraction: Extracts or preserves specified portions of date and time values.\n","permalink":"https://www.virture.de/posts/2023/de-identification-techniques/","summary":"Types of (PII) de-identification techniques Choosing the de-identification transformation you want to use depends on the kind of data you want to de-identify and for what purpose you\u0026rsquo;re de-identifying the data. The de-identification techniques that Sensitive Data Protection supports fall into the following general categories:\nRedaction: Deletes all or part of a detected sensitive value.\nReplacement: Replaces a detected sensitive value with a specified surrogate value.\nMasking: Replaces a number of characters of a sensitive value with a specified surrogate character, such as a hash (#) or asterisk (*).","title":"PII De-Identification techniques"},{"content":"Demystifying the Confusion Matrix: A Deep Dive into Evaluating Model Performance In the world of Machine Learning, understanding model performance is essential. One powerful tool for this purpose is the Confusion Matrix, a simple yet highly effective table layout for visualization and comprehension of your classifier\u0026rsquo;s performance.\nThe confusion matrix places the model\u0026rsquo;s predictions against the ground-truth data labels, creating an intuitive comparison grid. Each cell in this matrix represents a different aspect of the model\u0026rsquo;s performance, namely True Negatives (TN), False Negatives (FN), False Positives (FP) and True Positives (TP).\nUnderstanding the Confusion Matrix True Negatives (TN) The top-left cell in the matrix represents True Negatives. This indicates instances where the model predicted a negative result, and the actual label was indeed negative. In other words, these are the instances where the model correctly predicted a negative outcome.\nFalse Negatives (FN) The top-right cell represents False Negatives, indicating times when the model predicted a negative outcome, but the actual label was positive. This represents instances where the model should have predicted a positive outcome but did not.\nFalse Positives (FP) The bottom-left cell signifies False Positives. These are instances where the model predicted a positive outcome, but the actual label was negative. This denotes the occurrences where the model incorrectly predicted a positive outcome.\nTrue Positives (TP) Lastly, the bottom-right cell stands for True Positives, indicating times when the model correctly predicted a positive outcome.\nTo summarize, the key components of the confusion matrix are:\nTP = True positives: a positive label is correctly predicted. TN = True negatives: a negative label is correctly predicted. FP = False positives: a negative label is predicted as a positive. FN = False negatives: a positive label is predicted as a negative. Key Performance Metrics Derived from the Confusion Matrix Accuracy Accuracy is calculated as the number of correct predictions divided by the total number of predictions. It is a quick indicator of the overall performance of the model. However, accuracy may not be reliable when dealing with imbalanced datasets.\nMore generec accuracy fomularization:\naccuracy = number of correct predictions/total number of predictions Accuracy from pobability point of view:\naccuracy = (TP + TN) / number of samples Sensitivity/Recall (True Positive Rate) Sensitivity or Recall is a measure of the proportion of actual positive cases that were correctly identified by the model.\nrecall = sensitivity = TP / (TP + FN) Specificity Specificity denotes the fraction of actual negative cases that were correctly identified by the model.\nspecificity = TN / (TN + FP) Precision Precision represents the proportion of predicted positive cases that were correctly identified.\nprecision = TP / ( TP + FP) False Positive Rate (FPR) False Positive Rate denotes how often negative instances are incorrectly identified as positive.\nfalse_positive_rate = FP / ( FP + TN ) Key Note: The sum of specificity and false positive rate should always be 1!\nTrue Positive Rate (sensitivity) This metric illustrates how often \u0026ldquo;true\u0026rdquo; labels are correctly identified as \u0026ldquo;True\u0026rdquo;.\nFalse Positive Rate (False Alarm Rate) This illustrates how often \u0026ldquo;False\u0026rdquo; labels are incorrectly identified as \u0026ldquo;True\u0026rdquo;.\nWrapping Up The confusion matrix and its derived metrics provide a powerful and intuitive framework to evaluate the performance of classification models. By understanding each element and the relationships between them, we can gain valuable insights into our model\u0026rsquo;s strengths and weaknesses, and take steps to improve its performance.\n","permalink":"https://www.virture.de/posts/2023/confusion-matrix/","summary":"Demystifying the Confusion Matrix: A Deep Dive into Evaluating Model Performance In the world of Machine Learning, understanding model performance is essential. One powerful tool for this purpose is the Confusion Matrix, a simple yet highly effective table layout for visualization and comprehension of your classifier\u0026rsquo;s performance.\nThe confusion matrix places the model\u0026rsquo;s predictions against the ground-truth data labels, creating an intuitive comparison grid. Each cell in this matrix represents a different aspect of the model\u0026rsquo;s performance, namely True Negatives (TN), False Negatives (FN), False Positives (FP) and True Positives (TP).","title":"Confusion Matrix"},{"content":"Overview of Collaborative and Content-based Filtering in Recommender Systems Recommender systems have become integral to modern technology applications, spanning diverse industries from e-commerce to media streaming services. At the heart of these systems are filtering algorithms that power the delivery of personalized suggestions to each user. Two prominent types of filtering techniques are Collaborative Filtering and Content-based Filtering.\nKey Terms: Collaborative Filtering: This approach models recommendations based on a user\u0026rsquo;s past behavior, such as items previously purchased or selected, as well as numerical ratings given to those items. It also factors in the decisions made by other users who have similar patterns of behavior. The underlying premise is that users who agreed in the past will likely agree in the future. The model generated from this filtering method is then used to predict items or ratings for items that the user may find appealing.\nContent-based Filtering: In contrast, content-based filtering works by using discrete, pre-tagged characteristics of an item. For example, in a movie recommendation scenario, these characteristics could include the genre, the actors, the director, etc. This filtering approach then recommends additional items that possess similar properties to those that the user has shown a preference for in the past.\nSummary: In essence, Collaborative Filtering leverages collective user behavior to make suggestions, assuming that users with similar preferences in the past will continue to have similar preferences. Content-based Filtering, on the other hand, focuses on the properties of items themselves, suggesting items with similar characteristics to those the user has previously shown an interest in. Both approaches have their unique strengths and are employed based on the specific requirements of a recommender system.\n","permalink":"https://www.virture.de/posts/2023/collaborative-and-content-based-filtering/","summary":"Overview of Collaborative and Content-based Filtering in Recommender Systems Recommender systems have become integral to modern technology applications, spanning diverse industries from e-commerce to media streaming services. At the heart of these systems are filtering algorithms that power the delivery of personalized suggestions to each user. Two prominent types of filtering techniques are Collaborative Filtering and Content-based Filtering.\nKey Terms: Collaborative Filtering: This approach models recommendations based on a user\u0026rsquo;s past behavior, such as items previously purchased or selected, as well as numerical ratings given to those items.","title":"Collaborative and Content-based Filtering"},{"content":"Understanding Different Classification Loss Function Types in Machine Learning In machine learning, classification problems are quite common and central to many applications. The purpose of a classification problem is to predict discrete class labels, such as detecting if an email is spam or not, identifying the species of a flower based on measurements, or recognizing handwritten digits. While designing a classification model, a key part to consider is the choice of the loss function, which measures how well the model\u0026rsquo;s predictions align with the true values. The right choice of a loss function can guide your model towards better performance.\nIn this blog post, we will dive deep into various types of classification loss functions to help you make the best decision for your machine learning model.\nCategorical Hinge Loss This loss function is mainly used in Support Vector Machines (SVMs) with soft margins. Essentially, it calculates the distance between the actual and the predicted value and attempts to maximize the margin (the gap between the decision boundary and the closest data points from each class).\nTherefore, the smaller the categorical hinge loss, the larger the margin and the better your SVM performs. Binary Cross-Entropy Loss Binary cross-entropy loss, also known as log loss, is primarily used in binary classification problems, i.e., when there are only two classes to predict.\nThis loss function measures the dissimilarity between the true label and the predicted probability. One of its key features is that it heavily penalizes models that are confident about an incorrect classification. Categorical Cross-Entropy Loss Categorical cross-entropy loss is a generalization of binary cross-entropy loss and is used when there are more than two classes to predict. These classes do not necessarily have to be mutually exclusive.\nIt quantifies the distance between the actual and the predicted probability distribution. * As with binary cross-entropy, it applies a heavy penalty to confident and incorrect predictions. Sparse Categorical Cross-Entropy Loss Sparse categorical cross-entropy loss is another variant of the categorical cross-entropy loss but it is used for mutually exclusive multiclass classification problems.\nA notable advantage of this loss function is that it saves memory, making it particularly useful when dealing with large datasets. When to use which The key difference between categorical cross-entropy (cce) and sparse categorical cross-entropy (scce) lies in the format of the true and predicted class labels. Cce expects the labels to be one-hot encoded (a binary matrix representation of the class labels), which can be memory-inefficient when dealing with a large number of classes. On the other hand, scce works with integer labels, making it a memory-efficient alternative.\nCce loss function produces a one-hot array containing the probable match for each category, whereas scce loss function outputs the category index of the most likely matching category. This can lead to a significant reduction in memory usage when the number of categories is large. However, by using scce, you lose a lot of information about the probabilities of other classes, which might be important in some scenarios. In general, cce is preferred when reliability of the model is important. Nevertheless, there are situations when using scce can be beneficial:\nWhen your classes are mutually exclusive, meaning that each input only belongs to exactly one class. In this case, you don\u0026rsquo;t care at all about other close-enough predictions. When the number of categories is so large that storing the prediction output for all categories becomes infeasible or overwhelming.\nIn conclusion, selecting the right loss function is crucial for training an effective machine learning model. While this post discusses the loss functions used in classification tasks, there are many other loss functions out there suited to different types of machine learning tasks.\nAs always in machine learning, the choice of loss function should be guided by your specific problem and the nature of your data.\nSummary Classification loss functions are critical in guiding machine learning models towards optimal performance. The categorical hinge loss, predominantly used in Support Vector Machines, maximizes the margin for better model performance. Binary and categorical cross-entropy losses are used for binary and multi-class predictions respectively, heavily penalizing confident and incorrect predictions. Sparse categorical cross-entropy is suitable for mutually exclusive multi-class problems, offering memory efficiency, but sacrifices some information about other class probabilities.\nThe choice of loss function should be dictated by the specifics of your data and problem requirements.\n","permalink":"https://www.virture.de/posts/2023/classification-loss-functions/","summary":"Understanding Different Classification Loss Function Types in Machine Learning In machine learning, classification problems are quite common and central to many applications. The purpose of a classification problem is to predict discrete class labels, such as detecting if an email is spam or not, identifying the species of a flower based on measurements, or recognizing handwritten digits. While designing a classification model, a key part to consider is the choice of the loss function, which measures how well the model\u0026rsquo;s predictions align with the true values.","title":"Classification loss function types in ML"},{"content":"Azure Applied AI Services Azure Metrics Advisor\nAnalyze your business performance data and detect anomalies. Azure Cognitive Search\nEnrich data in your search indexes by using AI to analyze vision, language, and speech in content. Azure Immersive Reader\nImprove access to your web applications for new readers, language learners, and people with learning differences, such as dyslexia. Azure Bot Service\nCreate bots that can converse with your customers and partners and respond to their queries. Azure Video Analyzer\nGenerate data and business understanding from video streams. Azure Form Recognizer\nAzure Form Recognizer is an Azure service that you can use to analyze forms completed by your customers, partners, employers, or others and extract the data that they contain ","permalink":"https://www.virture.de/posts/2023/az-applied-ai/","summary":"Azure Applied AI Services Azure Metrics Advisor\nAnalyze your business performance data and detect anomalies. Azure Cognitive Search\nEnrich data in your search indexes by using AI to analyze vision, language, and speech in content. Azure Immersive Reader\nImprove access to your web applications for new readers, language learners, and people with learning differences, such as dyslexia. Azure Bot Service\nCreate bots that can converse with your customers and partners and respond to their queries.","title":"Azure AI Applied AI Services"},{"content":"Azure - AutoML Process Process Prepare data Train model Evaluate performance Deploy a predictive service Prepare Data Import data from Azure storage\nLocal files SQL databases Web files Azure Open Datasets Train model Classification (predicting categories or classes) Regression (predicting numeric values ) Time series forecasting (predicting numeric values at a future point in time) Natural language processing Computer vision Evaluate performance Cross validation RMSE (Root Mean Squared Error) NRMSE (Normalized Root Mean Squared Error) Residual History Predicted vs True chart Deploy Azure Container Instances (ACI) Azure Kubernetes Service (AKS) ","permalink":"https://www.virture.de/posts/2023/auto-ml/","summary":"Azure - AutoML Process Process Prepare data Train model Evaluate performance Deploy a predictive service Prepare Data Import data from Azure storage\nLocal files SQL databases Web files Azure Open Datasets Train model Classification (predicting categories or classes) Regression (predicting numeric values ) Time series forecasting (predicting numeric values at a future point in time) Natural language processing Computer vision Evaluate performance Cross validation RMSE (Root Mean Squared Error) NRMSE (Normalized Root Mean Squared Error) Residual History Predicted vs True chart Deploy Azure Container Instances (ACI) Azure Kubernetes Service (AKS) ","title":"Azure AutoML process"},{"content":"(YAST) A Guide to Passing the Microsoft AI-102 Certification How to lean for it..\nHello everyone,\nEmbarking on a journey to get certified can sometimes feel like a daunting endeavor. But worry not! This guide will share the most effective strategy I discovered for passing the Microsoft AI-102 Certification — most effective at least for my way of learning\u0026hellip;\nWhy This Guide? Everyone has their unique learning style. For me, it’s through reading and hands-on practice rather than video lectures. I find I can absorb the same amount of information much faster through reading than watching a video. Hence, I compiled this guide mainly based on text and hands-on practice resources.\n“My” Learning Path Microsoft Learn provides a wealth of resources, many of which are free. Here are the steps I followed in my preparation. I did not look into other resources, but I must admit, that I already have some experience with Azure as well as with ML and AI.\nStudy Guide https://learn.microsoft.com/en-us/certifications/resources/study-guides/ai-102#skills-measured-as-of-may-2-2023\nThis is your starting point. It provides crucial information about the test, including main topics, recent news, and changes. Keep a close eye on it to stay updated. In the Study Guide you will find links to the documentation of the relevant AI/ML services. It is certainly helpful, but not absolutely necessary, to read the entire documentation. But I definitely would recommend to carefully read at least the section marked below (Overview, concepts, Rest API, Python) of all relevant services.\nMust read documentation, example from Azure AI serice\nAI 900 Exercises https://microsoftlearning.github.io/AI-900-AIFundamentals/\nThese exercises are crucial for understanding the “core” ML/AI services. I used them as a “refresher.”\nAI 102 Learning Path https://learn.microsoft.com/en-us/certifications/exams/ai-102/#two-ways-to-prepare\nThis path provides a high-level view of all the services relevant for the exam. Work through all the sub-modules diligently and carry out the contained exercises.\nExample test from MSFT Learn On this link you can schedule you final exam as well as start a test exam for fre (multiple times): https://learn.microsoft.com/en-us/certifications/azure-ai-engineer/\nFree practice examen with answers — repeat this practice tests until you scored ≥ 80 % multiple times After working through the learning path, attempt the sample test at least once. This provides an idea of where you stand knowledge-wise and points you to additional resources for weaker areas.\nAI 104 Engineer Exercises + MSLearn Cognitive Services exercises https://microsoftlearning.github.io/AI-102-AIEngineer\nThese are must-have hands-on exercises. A significant part of the exam questions is based on these exercises.\nBuild and operate ML Learning Solutions from MSFT Learn\nhttps://learn.microsoft.com/en-us/training/paths/build-ai-solutions-with-azure-ml-service/\nAlthough it didn’t come up in my exam, this learning path provides a deeper understanding of MLOps under Azure.\nOpen AI Lessons (Optional) https://microsoftlearning.github.io/mslearn-openai/\nAlthough not yet part of the exam, these lessons provide useful additional practice.\nThe Exam In the exam, you’ll need to select upfront your peferred programming language (C# or Python). Questions about source code will then appear in that language. So you “just” have to learn to use the Azure AI/ML services in your preferred programming language. I went with Python..\nIt’s noteworthy that in my exam, about a third of the questions pertained to concrete API calls and source code. You’ll need to understand how to select code parts from drop-downs or choose the syntactically correct one from a series of possible API calls.\nConclusion Microsoft’s learning materials proved wholly sufficient for my successful exam preparation. Of course, having a basic knowledge of Python, Machine Learning, and Azure was helpful.\nI hope this guide will prove useful for you and encourage you to delve deeper into AI/ML on Azure using MSFT Learn lessons. The possibilities are indeed abundant.\nBest of luck with your exam preparations and remember, the journey is just as important as the destination!\n","permalink":"https://www.virture.de/posts/2023/ai-102-cert-leraning-path/","summary":"(YAST) A Guide to Passing the Microsoft AI-102 Certification How to lean for it..\nHello everyone,\nEmbarking on a journey to get certified can sometimes feel like a daunting endeavor. But worry not! This guide will share the most effective strategy I discovered for passing the Microsoft AI-102 Certification — most effective at least for my way of learning\u0026hellip;\nWhy This Guide? Everyone has their unique learning style. For me, it’s through reading and hands-on practice rather than video lectures.","title":"A guide to passing the Microsoft AI-102 Certification"},{"content":"Quickly reset VSCode to factory settings Introduction A handy guide on how to swiftly reset Visual Studio Code (VS Code) to its factory settings. Whether you\u0026rsquo;re experiencing performance issues, conflicts with extensions, or simply want a fresh start, resetting VS Code can often be the solution.\nWe\u0026rsquo;ll focus on resetting VS Code on macOS, where we\u0026rsquo;ll walk you through a series of commands to remove the necessary files and directories associated with VS Code. This includes removing the \u0026ldquo;.vscode\u0026rdquo; directory, clearing the application support, caches, and saved application state folders.\nBy following the steps outlined in this guide, you\u0026rsquo;ll be able to reset your VS Code installation and start with a clean slate. This can help resolve various issues and ensure a smooth and optimized coding experience.\nSo, if you\u0026rsquo;re ready to give your VS Code a fresh start and restore it to its default configuration, let\u0026rsquo;s dive in and discover how to quickly reset VS Code to factory settings!\nHere we go For macOS $ rm -rf \u0026#34;$HOME/.vscode\u0026#34; $ rm -rf \u0026#34;$HOME/Library/Application Support/Code\u0026#34; $ rm -rf \u0026#34;$HOME/Library/Caches/com.microsoft.VSCode\u0026#34; $ rm -rf \u0026#34;$HOME/Library/Saved Application State/com.microsoft.VSCode.savedState\u0026#34; found at: https://www.shellhacks.com/reset-visual-studio-code/\n","permalink":"https://www.virture.de/posts/2023/reset-vscode/","summary":"Quickly reset VSCode to factory settings Introduction A handy guide on how to swiftly reset Visual Studio Code (VS Code) to its factory settings. Whether you\u0026rsquo;re experiencing performance issues, conflicts with extensions, or simply want a fresh start, resetting VS Code can often be the solution.\nWe\u0026rsquo;ll focus on resetting VS Code on macOS, where we\u0026rsquo;ll walk you through a series of commands to remove the necessary files and directories associated with VS Code.","title":"Reset VSCode"},{"content":"CLI Login to azure for Terraform deployments Introduction In this article, we\u0026rsquo;ll explore the process of logging in to Azure for Terraform deployments and managing remote state. When working with Terraform to provision infrastructure in Azure, it\u0026rsquo;s crucial to establish a secure and authenticated connection to your Azure account.\nWe\u0026rsquo;ll start by discussing the steps to log in to Azure using the az login command. This command will prompt you to authenticate with your Azure credentials, allowing you to access and manage your Azure resources within your Terraform configuration.\nNext, we\u0026rsquo;ll delve into managing remote state in Terraform. If you have a tfstate file in a different subscription, resource group, or storage account, we\u0026rsquo;ll guide you through the process of exporting the necessary environment variable, ARM_ACCESS_KEY. This variable ensures that Terraform can authenticate and access the remote state file properly.\nBy following the instructions outlined in this guide, you\u0026rsquo;ll be able to log in to Azure for your Terraform deployments and seamlessly manage remote state, even if your tfstate file resides in a different subscription.\nRemark This approach should only be used for local development e.g via vscode and should not be used for CI/CD pipelines due to security risks\nHow to login to azure az login Set correct subscription az account set --subscription $SUBSCRIPTION_NAME If you have a tfstate in different subscription If you have an tfstate file in different subscriptio, resource group, or storage account you have to export the following environment variable:\nexport ARM_ACCESS_KEY=$(az storage account keys list --resource-group $RESGROUP --account-name $STORAGE_ACCOUNT_NAME --subscription $SUBSCRIPTION --query \u0026#39;[0].value\u0026#39; -o tsv) After this terraform init shoud work properly.\n","permalink":"https://www.virture.de/posts/2023/terraform-azure-login/","summary":"CLI Login to azure for Terraform deployments Introduction In this article, we\u0026rsquo;ll explore the process of logging in to Azure for Terraform deployments and managing remote state. When working with Terraform to provision infrastructure in Azure, it\u0026rsquo;s crucial to establish a secure and authenticated connection to your Azure account.\nWe\u0026rsquo;ll start by discussing the steps to log in to Azure using the az login command. This command will prompt you to authenticate with your Azure credentials, allowing you to access and manage your Azure resources within your Terraform configuration.","title":"Terraform Azure - Login and remote state"},{"content":"Linux Special characters in a shell (@, , €, ~) Introduction Let\u0026rsquo;s explore the usage of special characters in a shell environment, specifically focusing on Linux and macOS systems. These special characters, such as \u0026ldquo;@\u0026rdquo;, \u0026ldquo;\u0026rdquo;, \u0026ldquo;€\u0026rdquo;, \u0026ldquo;~\u0026rdquo;, and \u0026ldquo;|\u0026rdquo;, can be particularly useful when working with commands and scripts.\nHere we go right alt + Q = @ right alt + ß = right alt + ^ = | right alt + e = € right alt + n = ~\nalternatively:\nctrl + left alt + q = @ ctrl + left alt + ß = ctrl + left alt + ^ = | ctrl + left alt + e = € ctrl + left alt + n = ~~\nwith a mac keyboard\nopt + n = ~ opt + l = @ opt + 7 = | opt + shift + 7 = \\\nshow hidden files on macos:\nshift + command + .\nHave fun with different keyboard settings on different platforms especially when \u0026lsquo;flavored\u0026rsquo; with different language configurations - you\u0026rsquo;ll start loving complex passwords\u0026hellip;\n","permalink":"https://www.virture.de/posts/2023/linux-shell-special-chars/","summary":"Linux Special characters in a shell (@, , €, ~) Introduction Let\u0026rsquo;s explore the usage of special characters in a shell environment, specifically focusing on Linux and macOS systems. These special characters, such as \u0026ldquo;@\u0026rdquo;, \u0026ldquo;\u0026rdquo;, \u0026ldquo;€\u0026rdquo;, \u0026ldquo;~\u0026rdquo;, and \u0026ldquo;|\u0026rdquo;, can be particularly useful when working with commands and scripts.\nHere we go right alt + Q = @ right alt + ß = right alt + ^ = | right alt + e = € right alt + n = ~","title":"Special Chars in a Linux Shell"},{"content":"Setup an python environment in vscode Introduction In this article, we\u0026rsquo;ll explore a step-by-step guide on how to swiftly set up a Python environment in Visual Studio Code (VS Code). Whether you\u0026rsquo;re a Python beginner or a seasoned developer, having an efficient and organized development environment is crucial for productive coding sessions.\nWe\u0026rsquo;ll start by creating a new environment using the Python virtual environment module. By isolating our project dependencies, we ensure a clean and controlled environment. We\u0026rsquo;ll cover the necessary commands to create a new environment directory and activate it.\nOnce our environment is activated, we\u0026rsquo;ll dive into managing package installations. You\u0026rsquo;ll learn how to list the installed packages using pip and even install specific versions of libraries like pandas. Additionally, we\u0026rsquo;ll reveal a handy trick to generate a requirements.txt file on the fly, simplifying the process of documenting project dependencies.\nBy the end of this guide, you\u0026rsquo;ll have a streamlined Python environment set up in VS Code, ready to tackle your coding challenges efficiently. So, let\u0026rsquo;s get started and empower your Python development workflow in no time!\nLet\u0026rsquo;s dig into it: Setup an python environment in vscode In shell:\nCreating new environment python3 -m venv \u0026lt;new-dir\u0026gt; Activating environment source \u0026lt;new-dir\u0026gt;/bin/activate List installed packages pip3 list Deactivate environment with: deactivate Install a specific version of a lib: pip3 install pandas==1.2.5 Create requirements.txt on the fly There are more fancy aproaches, but this one is very quick:\npip freeze \u0026gt;\u0026gt; requirements.txt ","permalink":"https://www.virture.de/posts/2023/python-environment/","summary":"Setup an python environment in vscode Introduction In this article, we\u0026rsquo;ll explore a step-by-step guide on how to swiftly set up a Python environment in Visual Studio Code (VS Code). Whether you\u0026rsquo;re a Python beginner or a seasoned developer, having an efficient and organized development environment is crucial for productive coding sessions.\nWe\u0026rsquo;ll start by creating a new environment using the Python virtual environment module. By isolating our project dependencies, we ensure a clean and controlled environment.","title":"Python environment"},{"content":"How to get own external IP via terminal (curl) This command even provides some additional interesting information:\ncurl ipinfo.io If you prefer the \u0026ldquo;Way of the Browser\u0026rdquo;: https://whatismyipaddress.com/\n","permalink":"https://www.virture.de/posts/2023/get-external-ip/","summary":"How to get own external IP via terminal (curl) This command even provides some additional interesting information:\ncurl ipinfo.io If you prefer the \u0026ldquo;Way of the Browser\u0026rdquo;: https://whatismyipaddress.com/","title":"Getting your own external IP via curl"},{"content":"Publish a website or a blog on github pages Introduction: Are you looking to share your website with the world? GitHub Pages provides an easy and free way to publish your static website directly from your GitHub repository. In this blog post, we\u0026rsquo;ll walk you through the process of publishing your website on GitHub Pages, allowing you to showcase your work and make it accessible to a global audience.\nStep 1: Create a Repository: Start by creating a new repository on GitHub. Ensure that you name it in the following format: .github.io. This naming convention is essential for GitHub Pages to recognize it as your personal website.\nStep 2: Add Your Website Files: Once your repository is created, add your website files to the repository. Ensure that your website is a static site, as GitHub Pages only supports hosting static content. You can include HTML, CSS, JavaScript, and any other necessary assets.\nStep 3: Enable GitHub Pages: Now, navigate to the repository\u0026rsquo;s settings page. Scroll down to the \u0026ldquo;GitHub Pages\u0026rdquo; section. Under the \u0026ldquo;Source\u0026rdquo; heading, select the branch that contains your website files. Typically, it will be the \u0026ldquo;main\u0026rdquo; branch. Select the folder in which your root is placed, e.g. index.html. It should lokke like this (in this example the subflder \u0026lsquo;docs\u0026rsquo; contains the websites root)\nStep 4: Publish Your Website: After enabling GitHub Pages and selecting the source branch, click on the provided link to access your published website. It might take a few moments for your changes to propagate. Congratulations! Your website is now live on GitHub Pages.\nStep 6: Update Your Website: As you make updates to your website, commit and push the changes to your repository. GitHub Pages will automatically reflect those changes, allowing you to iterate and improve your site seamlessly.\nConclusion: Publishing your website on GitHub Pages provides a simple and convenient way to share your work with the world. By following the steps outlined in this guide, you can effortlessly showcase your static website to a global audience.\n","permalink":"https://www.virture.de/posts/2023/publish-website-github/","summary":"Publish a website or a blog on github pages Introduction: Are you looking to share your website with the world? GitHub Pages provides an easy and free way to publish your static website directly from your GitHub repository. In this blog post, we\u0026rsquo;ll walk you through the process of publishing your website on GitHub Pages, allowing you to showcase your work and make it accessible to a global audience.\nStep 1: Create a Repository: Start by creating a new repository on GitHub.","title":"Publish a website on github pages"},{"content":"How to enable azure cli autocomplete in macos zsh shell Azure-cli comes with an autocomplete script that should be located in\n/usr/local/etc/bash_completion.d*´ directory. The particular scritp is named \u0026lsquo;az\u0026rsquo;\nFor enabling autocomplite you just have to source that script:\nsource /usr/local/etc/bash_completion.d/az Enable azure cli autocomletion by default Add the line zo you .zshrc file:\necho \u0026#34;source /usr/local/etc/bash_completion.d/az\u0026#34; \u0026gt;\u0026gt; ~/.szhrc If you encounter an error when restarting, you may have to add that line before the script loading to ensure cross compatibility between bash and zsh script\nautoload -U +X bashcompinit \u0026amp;\u0026amp; bashcompinit To relaod .zshrc, simlpy source it:\nsource ~/.zshrc That\u0026rsquo;s it!\n","permalink":"https://www.virture.de/posts/2023/azure-cli-autocomplete/","summary":"How to enable azure cli autocomplete in macos zsh shell Azure-cli comes with an autocomplete script that should be located in\n/usr/local/etc/bash_completion.d*´ directory. The particular scritp is named \u0026lsquo;az\u0026rsquo;\nFor enabling autocomplite you just have to source that script:\nsource /usr/local/etc/bash_completion.d/az Enable azure cli autocomletion by default Add the line zo you .zshrc file:\necho \u0026#34;source /usr/local/etc/bash_completion.d/az\u0026#34; \u0026gt;\u0026gt; ~/.szhrc If you encounter an error when restarting, you may have to add that line before the script loading to ensure cross compatibility between bash and zsh script","title":"Azure CLI - activate azure cli autocomplete in zsh"}]