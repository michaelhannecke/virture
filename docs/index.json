[{"content":"Overview about actual AI and ML Services on Google Cloud (September 2023) Service Description Vertex AI A unified platform for building, training, deploying, and managing machine learning models. Vertex AI Notebooks A managed service for running Jupyter notebooks, providing a single interface for your data, analytics, and machine learning workflow. Vertex Explainable AI Tools and frameworks to help you understand and interpret your machine learning models. AI Platform Prediction A managed service for deploying and managing machine learning models, making it easy to serve predictions to your applications and users. AI Platform Training A managed service for training machine learning models, providing a variety of training options and resources. AI Platform Data Labeling A managed service for labeling data to train machine learning models, providing a variety of labeling tools and options. AI Platform Custom Training A managed service for training custom machine learning models, providing a variety of training options and resources. AI Platform Custom Prediction A managed service for deploying and managing custom machine learning models, making it easy to serve predictions to your applications and users. AI Platform Prediction Service A deprecated service that has been replaced by Vertex AI Prediction. AI Platform Training Service A deprecated service that has been replaced by Vertex AI Training. AI Platform Data Labeling Service A deprecated service that has been replaced by Vertex AI Data Labeling. AI Platform Custom Training Service A deprecated service that has been replaced by Vertex AI Custom Training. AI Platform Custom Prediction Service A deprecated service that has been replaced by Vertex AI Custom Prediction. Additional services:\nService Description AutoML Vision A machine learning service that trains custom image classification models quickly and easily. AutoML Natural Language A machine learning service that trains custom text classification, translation, and entity extraction models quickly and easily. AutoML Tables A machine learning service that trains custom machine learning models for structured data prediction tasks, such as classification, regression, and forecasting. Cloud Machine Learning Engine A deprecated service that has been replaced by Vertex AI Training. Cloud AutoML A deprecated service that has been replaced by Vertex AI AutoML Vision, Vertex AI AutoML Natural Language, and Vertex AI AutoML Tables. Cloud Natural Language API A natural language processing API that provides text analysis, translation, and speech-to-text and text-to-speech services. Cloud Speech-to-Text A speech recognition API that converts audio to text. Cloud Text-to-Speech A text-to-speech API that converts text to audio. Cloud Vision API A computer vision API that provides image analysis and object detection services. Cloud Video Intelligence API A video intelligence API that provides video analysis and object detection services. Cloud Dialogflow A natural language conversational AI service that allows you to build chatbots and virtual assistants. Cloud Translation API A machine translation API that translates text from one language to another. Cloud Healthcare API A healthcare API that provides natural language processing and machine learning services for healthcare data. Remark: This list list shows only services which are general available (GA). Services in preview are not listet here.\n","permalink":"https://www.virture.de/posts/2023/gcp-ai-ml-services/","summary":"Overview about actual AI and ML Services on Google Cloud (September 2023) Service Description Vertex AI A unified platform for building, training, deploying, and managing machine learning models. Vertex AI Notebooks A managed service for running Jupyter notebooks, providing a single interface for your data, analytics, and machine learning workflow. Vertex Explainable AI Tools and frameworks to help you understand and interpret your machine learning models. AI Platform Prediction A managed service for deploying and managing machine learning models, making it easy to serve predictions to your applications and users.","title":"GCP AI and ML Services Overview"},{"content":"Overview about actual AI and ML Services on microsoft Azure (September 2023) Service Description Azure Machine Learning A cloud-based platform for building, training, deploying, and managing machine learning models. Azure Synapse Analytics An analytics platform that combines data warehousing, big data analytics, and machine learning. Azure Databricks A unified analytics platform that combines data science, engineering, and business intelligence. Azure Cognitive Services A set of AI services that enable developers to add cognitive capabilities to their applications. Azure Applied AI Services A set of AI services that are tailored to specific industries and use cases. Azure Quantum A quantum computing platform that enables developers to build and run quantum applications. Additional services:\nService Description Azure Machine Learning designer A no-code/low-code machine learning tool that enables users to build and deploy machine learning models without writing code. Azure Machine Learning automated ML A machine learning automation service that enables users to build and deploy machine learning models with minimal coding required. Azure Machine Learning responsible AI A set of tools and resources to help users build, deploy, and manage responsible AI solutions. Azure Machine Learning MLOps A set of tools and resources to help users implement MLOps practices in their machine learning workflows. Azure Synapse Analytics Machine Learning A set of machine learning capabilities embedded within Azure Synapse Analytics. Azure Databricks Machine Learning A set of machine learning capabilities embedded within Azure Databricks. Cognitive Services Computer Vision A service that enables developers to add image analysis and object detection capabilities to their applications. Cognitive Services Custom Vision A service that enables developers to train and deploy custom image classification models. Cognitive Services Face A service that enables developers to add face detection, analysis, and recognition capabilities to their applications. Cognitive Services Speech A service that enables developers to add speech recognition and synthesis capabilities to their applications. Cognitive Services Text Analytics A service that enables developers to add natural language processing capabilities to their applications. Cognitive Services Translator A service that enables developers to add machine translation capabilities to their applications. Applied AI Services:\nService Description Azure Bot Service A service that enables developers to build and deploy chatbots. Azure Form Recognizer A service that enables developers to extract data from forms and documents. Azure Healthcare AI A set of AI services tailored to the healthcare industry. Azure Industrial AI A set of AI services tailored to the manufacturing industry. Azure Retail AI A set of AI services tailored to the retail industry. Remark: This list list shows only services which are general available (GA). Services in preview are not listet here.\n","permalink":"https://www.virture.de/posts/2023/azure-ai-ml-services/","summary":"Overview about actual AI and ML Services on microsoft Azure (September 2023) Service Description Azure Machine Learning A cloud-based platform for building, training, deploying, and managing machine learning models. Azure Synapse Analytics An analytics platform that combines data warehousing, big data analytics, and machine learning. Azure Databricks A unified analytics platform that combines data science, engineering, and business intelligence. Azure Cognitive Services A set of AI services that enable developers to add cognitive capabilities to their applications.","title":"Azure AI and ML Services Overview"},{"content":"Overview about actual AI and ML Services on Amazon AWS (September 2023) Service Description Amazon SageMaker A managed machine learning (ML) service that helps data scientists and developers build, train, deploy, and manage ML models. Amazon Rekognition An image and video analysis service that provides facial recognition, object detection, scene detection, and image classification capabilities. Amazon Comprehend A natural language processing (NLP) service that provides text analysis, sentiment analysis, entity extraction, and topic modeling capabilities. Amazon Polly A text-to-speech (TTS) service that converts text into lifelike speech. Amazon Lex A chatbot building service that allows developers to build conversational AI applications. Amazon Personalize A recommendation and personalization service that helps developers create personalized experiences for their customers. Amazon Translate A machine translation service that translates text from one language to another. Amazon Transcribe A speech-to-text (STT) service that converts speech into text. Amazon Comprehend Medical An NLP service that provides medical text analysis, such as entity extraction and sentiment analysis, for healthcare data. Amazon Forecast A forecasting service that uses machine learning to predict future outcomes. Amazon Fraud Detector A fraud detection service that uses machine learning to identify fraudulent transactions. Amazon Rekognition Custom Labels A service that allows developers to create custom labels for Amazon Rekognition. Amazon Personalize Custom Events A service that allows developers to create custom events for Amazon Personalize. Amazon Forecast Custom Models A service that allows developers to create custom forecasting models for Amazon Forecast. Amazon Fraud Detector Custom Models A service that allows developers to create custom fraud detection models for Amazon Fraud Detector. Additional services:\nService Description Amazon SageMaker Ground Truth A managed data labeling service that helps developers label data for machine learning models. Amazon SageMaker Neo A runtime environment that helps developers deploy machine learning models to mobile devices and edge devices. Amazon SageMaker Model Monitor A service that helps developers monitor the performance of their machine learning models in production. Amazon SageMaker Autopilot A managed machine learning service that automates the machine learning process, from data preparation to model deployment. Amazon SageMaker Studio An integrated development environment (IDE) for machine learning that provides a single interface for data scientists and developers to build, train, and deploy machine learning models. Remark: This list list shows only services which are general available (GA). Services in preview are not listet here.\n","permalink":"https://www.virture.de/posts/2023/aa-aws-ai-ml-services-copy/","summary":"Overview about actual AI and ML Services on Amazon AWS (September 2023) Service Description Amazon SageMaker A managed machine learning (ML) service that helps data scientists and developers build, train, deploy, and manage ML models. Amazon Rekognition An image and video analysis service that provides facial recognition, object detection, scene detection, and image classification capabilities. Amazon Comprehend A natural language processing (NLP) service that provides text analysis, sentiment analysis, entity extraction, and topic modeling capabilities.","title":"AWS AI and ML Services Overview"},{"content":"Create a Kubernetes managed Cluster on Google GCP with one line of code !!Be Careful: running ths may incur costs on your project - make sure to delete the resources when finished!!\nprerequisits A project with Kubernetes API enabled GCP CLI installed An environment variable CLUSTER_NAME defined with the name of the cluster Compute/Region set in project configuration to your preferred Region default VPC in place. gcloud config set compute/zone \u0026lt;preferred-region\u0026gt; PROJECT_ID=$(gcloud config get-value project) CLUSTER_NAME=\u0026lt;cluster-name\u0026gt; The Code : gcloud beta container clusters create $CLUSTER_NAME \\ --cluster-version=latest \\ --machine-type=e2-standard-4 \\ --enable-autoscaling \\ --min-nodes=1 \\ --max-nodes=3 \\ --num-nodes=1 Get Credentials gcloud container sclusters get-credentials $CLUSTER_NAME Check pods\nkubectl get pods -A Delete cluster Delte the resources when not longer needed:\ncloud container clusters delete $CLUSTER_NAME ","permalink":"https://www.virture.de/posts/2023/kubernetes-on-gcp-onliner/","summary":"Create a Kubernetes managed Cluster on Google GCP with one line of code !!Be Careful: running ths may incur costs on your project - make sure to delete the resources when finished!!\nprerequisits A project with Kubernetes API enabled GCP CLI installed An environment variable CLUSTER_NAME defined with the name of the cluster Compute/Region set in project configuration to your preferred Region default VPC in place. gcloud config set compute/zone \u0026lt;preferred-region\u0026gt; PROJECT_ID=$(gcloud config get-value project) CLUSTER_NAME=\u0026lt;cluster-name\u0026gt; The Code : gcloud beta container clusters create $CLUSTER_NAME \\ --cluster-version=latest \\ --machine-type=e2-standard-4 \\ --enable-autoscaling \\ --min-nodes=1 \\ --max-nodes=3 \\ --num-nodes=1 Get Credentials gcloud container sclusters get-credentials $CLUSTER_NAME Check pods","title":"Kubernetes - create a cluster with one line"},{"content":"Start scripts for kubernetes native nodes These statup scripts require a UBUNTU 20.04 LTS base image!\nIf you want to use Ubuntun 22.04 LTS, exchange for the kubenretes part \u0026lsquo;xenial\u0026rsquo; with \u0026lsquo;jummy\u0026rsquo;.\nScript for the master node on AWS #!/bin/bash # Load necessary kernel modules for containerd cat \u0026lt;\u0026lt; EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # Configure kernel networking requirements for Kubernetes cat \u0026lt;\u0026lt; EOF | sudo tee /etc/sysctl.d/99-kubernetes.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system # Update package information and install containerd sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y containerd # Generate and set containerd configuration sudo mkdir -p /etc/containerd sudo containerd config default | sudo tee /etc/containerd/config.toml # Restart containerd with the new configuration sudo systemctl restart containerd # Disable swap to meet Kubernetes requirements sudo swapoff -a sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl # Add Kubernetes apt repository and GPG key curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmour -o /usr/share/keyrings/kubernetes.gpg echo \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/kubernetes.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/kubernetes.list # Update package information and install specific Kubernetes components sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubelet=1.28.0-00 kubeadm=1.28.0-00 kubectl=1.28.0-00 # Prevent automatic updates for Kubernetes components sudo apt-mark hold kubelet kubeadm kubectl # Initialize the Kubernetes control plane with specified settings sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.28.0 # Configure kubeconfig for the current user mkdir -p /home/ubuntu/.kube \u0026gt;\u0026gt; /var/log/startup.log 2\u0026gt;\u0026amp;1 sudo cp -i /etc/kubernetes/admin.conf /home/ubuntu/.kube/config \u0026gt;\u0026gt; /var/log/startup.log 2\u0026gt;\u0026amp;1 sudo chown $(id -u ubuntu):$(id -g ubuntu) /home/ubuntu/.kube/config \u0026gt;\u0026gt; /var/log/startup.log 2\u0026gt;\u0026amp;1 # Install Calico network plugin for pod networking kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml \u0026gt;\u0026gt; calico-setup.log # Generate join command for worker nodes sudo kubeadm token create --print-join-command \u0026gt;\u0026gt; /tmp/join-command.txt Slightly different Startup Script for the master Node on GCP this is due to the slighlty different user setup on GCP. You have to change the last lines of the script above:\n## CHANGE THIS LINE for GCP virtual machine: sudo -u ubuntu /usr/bin/kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml \u0026gt;\u0026gt; /tmp/startup.log 2\u0026gt;\u0026amp;1 Script for a worker node (both AWS and GCP) #!/bin/bash # Load necessary kernel modules for containerd cat \u0026lt;\u0026lt; EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # Configure kernel networking requirements for Kubernetes cat \u0026lt;\u0026lt; EOF | sudo tee /etc/sysctl.d/99-kubernetes.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system # Update package information and install containerd sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y containerd # Generate and set containerd configuration sudo mkdir -p /etc/containerd sudo containerd config default | sudo tee /etc/containerd/config.toml # Restart containerd with the new configuration sudo systemctl restart containerd # Disable swap to meet Kubernetes requirements sudo swapoff -a sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl # Add Kubernetes apt repository and GPG key curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmour -o /usr/share/keyrings/kubernetes.gpg echo \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/kubernetes.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/kubernetes.list # Update package information and install specific Kubernetes components sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubelet=1.28.0-00 kubeadm=1.28.0-00 kubectl=1.28.0-00 # Prevent automatic updates for Kubernetes components sudo apt-mark hold kubelet kubeadm kubectl Have fun!\n","permalink":"https://www.virture.de/posts/2023/kubernetes-nodes-setup/","summary":"Start scripts for kubernetes native nodes These statup scripts require a UBUNTU 20.04 LTS base image!\nIf you want to use Ubuntun 22.04 LTS, exchange for the kubenretes part \u0026lsquo;xenial\u0026rsquo; with \u0026lsquo;jummy\u0026rsquo;.\nScript for the master node on AWS #!/bin/bash # Load necessary kernel modules for containerd cat \u0026lt;\u0026lt; EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # Configure kernel networking requirements for Kubernetes cat \u0026lt;\u0026lt; EOF | sudo tee /etc/sysctl.","title":"kubernetes nodes setup"},{"content":"AWS multiple accounts/profiles If you\u0026rsquo;re working with multiple AWS accounts or environments, using named profiles with the AWS CLI can be very handy. Named profiles are configurations of AWS access keys, secret keys, and default regions. Here\u0026rsquo;s how you can set up and use multiple profiles:\nSetting Up Multiple Profiles You can add profiles directly to your AWS configuration file (~/.aws/config) and your credentials file (~/.aws/credentials).\nHere\u0026rsquo;s an example for ~/.aws/credentials:\n[default] aws_access_key_id = YOUR_DEFAULT_ACCESS_KEY aws_secret_access_key = YOUR_DEFAULT_SECRET_KEY [profileName1] aws_access_key_id = YOUR_ACCESS_KEY_FOR_PROFILE1 aws_secret_access_key = YOUR_SECRET_KEY_FOR_PROFILE1 [profileName2] aws_access_key_id = YOUR_ACCESS_KEY_FOR_PROFILE2 aws_secret_access_key = YOUR_SECRET_KEY_FOR_PROFILE2 And for ~/.aws/config:\n[default] region = us-west-1 [profile profileName1] region = us-east-1 [profile profileName2] region = eu-central-1 Alternatively, you can use the AWS CLI to configure these profiles:\naws configure --profile profileName1 This command will prompt you for the necessary details for profileName1.\nUsing a Specific Profile To use a specific profile with the AWS CLI, use the \u0026ndash;profile flag:\naws s3 ls --profile profileName1 ","permalink":"https://www.virture.de/posts/2023/aws-multiple-profiles/","summary":"AWS multiple accounts/profiles If you\u0026rsquo;re working with multiple AWS accounts or environments, using named profiles with the AWS CLI can be very handy. Named profiles are configurations of AWS access keys, secret keys, and default regions. Here\u0026rsquo;s how you can set up and use multiple profiles:\nSetting Up Multiple Profiles You can add profiles directly to your AWS configuration file (~/.aws/config) and your credentials file (~/.aws/credentials).\nHere\u0026rsquo;s an example for ~/.","title":"AWS multiple accounts profiles with cli/terraform"},{"content":"Autoregressive integrated moving average (ARIMA) Autoregressive integrated moving average (ARIMA) is a statistical model that is used to forecast future values of a time series. It is a generalization of the autoregressive moving average (ARMA) model, which only considers the autocorrelations between the current value and the past values of the time series. ARIMA also considers the moving average of the residuals of the ARMA model, which helps to improve the accuracy of the forecasts.\nARIMA models the time series as a combination of three components:\nAutoregression (AR): The AR component models the dependence between the current value and the past values of the time series.\nMoving average (MA): The MA component models the dependence between the current value and the errors of the previous predictions.\nIntegration (I): The I component models the non-stationarity of the time series by differencing the data.\nThe ARIMA model is specified by three integers: p, d, and q.\np: The number of lags in the AR component.\nd: The number of times the data is differenced to make it stationary.\nq: The number of lags in the MA component.\nARIMA models are commonly used to forecast financial data, such as stock prices and exchange rates. They are also used to forecast other types of time series data, such as customer demand, sales, and inventory levels.\nThe ARIMA model can be used to forecast the stock price for a specific date, or it can be used to forecast the stock price over a period of time. The forecast can be used to make investment decisions, such as whether to buy or sell the stock.\nARIMA models are a powerful tool for forecasting time series data. They are relatively easy to understand and implement, and they can be used to forecast a variety of time series data.\n","permalink":"https://www.virture.de/posts/2023/arima/","summary":"Autoregressive integrated moving average (ARIMA) Autoregressive integrated moving average (ARIMA) is a statistical model that is used to forecast future values of a time series. It is a generalization of the autoregressive moving average (ARMA) model, which only considers the autocorrelations between the current value and the past values of the time series. ARIMA also considers the moving average of the residuals of the ARMA model, which helps to improve the accuracy of the forecasts.","title":"Auotoregressive Intregrated Moving Average (ARIMA)"},{"content":"Softmax Activation Function The softmax activation function is a non-linear function that is commonly used in the output layer of neural networks for multi-class classification problems. It takes a vector of real numbers as input and outputs a vector of probabilities, where the probabilities sum to 1. This means that the softmax function can be used to represent a probability distribution over the possible output classes.\nThe softmax function is defined as follows:\nsoftmax(x) = [exp(x_1) / sum(exp(x_i)) for x_i in x] where x is the vector of input values and exp(x) is the exponential function. The softmax function first exponentiates each element of the input vector, which makes them all positive. It then divides each exponentiated value by the sum of all the exponentiated values. This ensures that the output vector of probabilities sums to 1.\nThe softmax function can be interpreted as a way of normalizing the output of a neural network so that it represents a probability distribution. This is useful for multi-class classification problems, where the goal is to predict the probability of a given input belonging to each of the possible output classes.\nExample of how the softmax function can be used to classify images Suppose we have a neural network that has been trained to classify images of cats and dogs. The output layer of the neural network will have two neurons, one for each output class. The softmax function will be applied to the output of these neurons to produce a vector of two probabilities, one for the probability of the input image being a cat and one for the probability of the input image being a dog. The neuron with the highest probability will be the predicted class for the input image.\nThe softmax function is a powerful and versatile activation function that is commonly used in neural networks for multi-class classification problems. It is easy to implement and understand, and it produces outputs that are easy to interpret.\n","permalink":"https://www.virture.de/posts/2023/softmax/","summary":"Softmax Activation Function The softmax activation function is a non-linear function that is commonly used in the output layer of neural networks for multi-class classification problems. It takes a vector of real numbers as input and outputs a vector of probabilities, where the probabilities sum to 1. This means that the softmax function can be used to represent a probability distribution over the possible output classes.\nThe softmax function is defined as follows:","title":"SoftMax Activation Function"},{"content":"Analytics Random Cut Forest (RCF) Analytics Random Cut Forest (RCF) is an unsupervised machine learning algorithm that is used for anomaly detection. In RCF, each tree is constructed by randomly selecting a subset of features and then randomly selecting a subset of data points from the training dataset. This process helps to ensure that the trees are more diverse and less correlated, which makes them more effective at detecting anomalies.\nOne example of a use case for RCF is anomaly detection in time series data.\nAnother example of a use case for RCF is fraud detection. Here are some additional details about RCF:\nIt is a fast and efficient algorithm that can be used to process large datasets.\nIt is relatively easy to interpret and understand.\nIt is effective at detecting both point anomalies and contextual anomalies.\nIt can be used to detect anomalies in a variety of data types, including time series data, sensor data, and financial data.\n","permalink":"https://www.virture.de/posts/2023/rcf/","summary":"Analytics Random Cut Forest (RCF) Analytics Random Cut Forest (RCF) is an unsupervised machine learning algorithm that is used for anomaly detection. In RCF, each tree is constructed by randomly selecting a subset of features and then randomly selecting a subset of data points from the training dataset. This process helps to ensure that the trees are more diverse and less correlated, which makes them more effective at detecting anomalies.","title":"Analytics Random Cut Forest (RCF)"},{"content":"Horovod Distributed Training Framework Horovod is a distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. It is designed to make distributed training easy and efficient. Horovod uses a ring-based communication pattern to efficiently distribute data across multiple GPUs or machines. This can significantly improve the training speed of deep learning models.\nHorovod is also designed to be easy to use. It can be used with existing TensorFlow, Keras, PyTorch, and Apache MXNet code with minimal changes. Horovod provides a number of features that make distributed training easier, such as:\nAutomatic scaling: Horovod can automatically scale your training jobs to use more GPUs or machines as needed. This can help you to achieve the best possible training speed for your model. Fault tolerance: Horovod can recover from failures of individual GPUs or machines. This means that your training jobs will not be interrupted if a GPU or machine fails. Logging and monitoring: Horovod provides a number of tools for logging and monitoring your training jobs. This can help you to track the progress of your training and identify any problems. Overall, Horovod is a powerful and easy-to-use distributed training framework that can significantly improve the training speed of deep learning models. It is a good choice for businesses that need to train large and complex deep learning models.\nHere are some of the benefits of using Horovod:\nSpeed: Horovod can significantly improve the training speed of deep learning models. This is because it uses a ring-based communication pattern to efficiently distribute data across multiple GPUs or machines.\nEase of use: Horovod is designed to be easy to use. It can be used with existing TensorFlow, Keras, PyTorch, and Apache MXNet code with minimal changes.\nFault tolerance: Horovod can recover from failures of individual GPUs or machines. This means that your training jobs will not be interrupted if a GPU or machine fails.\nLogging and monitoring: Horovod provides a number of tools for logging and monitoring your training jobs. This can help you to track the progress of your training and identify any problems.\n","permalink":"https://www.virture.de/posts/2023/horovord-distribution/","summary":"Horovod Distributed Training Framework Horovod is a distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. It is designed to make distributed training easy and efficient. Horovod uses a ring-based communication pattern to efficiently distribute data across multiple GPUs or machines. This can significantly improve the training speed of deep learning models.\nHorovod is also designed to be easy to use. It can be used with existing TensorFlow, Keras, PyTorch, and Apache MXNet code with minimal changes.","title":"Horovod Distributed Training Framework"},{"content":"What are the reasons to use machine learning? An example of a business problem where the use of ML would be appropriate is generating personalized recommendations. In this case, the solution to the problem requires complex logic, and we would want to provide personalized recommendations at scale with quick turnaround times.\nRequires complex logic Since developing personalized recommendations requires complex logic, ML is an appropriate tool to consider. Requires scalability Serving millions of requests for personalized recommendations every second is a challenge. Requires personalization Delivering personalized recommendations at scale and being responsive at the same time is difficult to achieve with classical programming techniques. Requires responsiveness The ability to deliver personalized recommendations within a few seconds even while handling millions of requests per second is expected. What are the reasons to NOT use machine learning? Business reasons to avoid ML depend on whether traditional methods and rules are viable options, if there are few or no requirements to adapt to new data, if business goals include 100% outcome accuracy, or if models must be explained or translated.\nCan be solved with traditional algorithms If the problem is not overly complex, an ML solution might be overcomplicated Does not require adapting to new data If data and conditions are not changing, a more traditional approach could be more appropriate. Requires 100% accuracy ML predictions often provide less than 100% accuracy. Requires full interpretability If being able to explain what is going to happen if you change the parameters or input is a priority, ML might not be the best solution. ","permalink":"https://www.virture.de/posts/2023/when-to-use-ml/","summary":"What are the reasons to use machine learning? An example of a business problem where the use of ML would be appropriate is generating personalized recommendations. In this case, the solution to the problem requires complex logic, and we would want to provide personalized recommendations at scale with quick turnaround times.\nRequires complex logic Since developing personalized recommendations requires complex logic, ML is an appropriate tool to consider. Requires scalability Serving millions of requests for personalized recommendations every second is a challenge.","title":"When to use ML and when not"},{"content":"Methods to prevent overfitting in Machine Laerning L2 Regularization (Ridge Regression) L2 regularization adds a penalty term to the loss function based on the squared magnitudes of the model\u0026rsquo;s weights. This penalty discourages large weight values and encourages the model to use smaller weights, leading to a smoother and more generalized solution. The regularization term is controlled by a hyperparameter (lambda or alpha) that balances the trade-off between fitting the training data and keeping the weights small.\nL1 Regularization (Lasso Regression) L1 regularization adds a penalty term to the loss function based on the absolute magnitudes of the model\u0026rsquo;s weights. Similar to L2 regularization, L1 regularization also encourages the model to use smaller weights, but it has the additional property of driving some weights to exactly zero. This makes L1 regularization useful for feature selection and creating sparse models.\nOther methods to prevent overfitting include:\nDropout Dropout is a technique used primarily in neural networks. During training, random neurons are temporarily dropped out (set to zero) with a given probability. This forces the network to learn more robust features and prevents it from relying too heavily on specific neurons, thereby reducing overfitting.\nCross-Validation Cross-validation is a technique used to assess the model\u0026rsquo;s performance on different subsets of the data. It helps in evaluating how well the model generalizes to new data and can provide insights into overfitting issues.\nEarly Stopping Early stopping involves monitoring the model\u0026rsquo;s performance on a validation set during training. Training is stopped when the model\u0026rsquo;s performance on the validation set starts to degrade, preventing it from overfitting the training data.\nData Augmentation Data augmentation involves generating additional training data by applying random transformations (e.g., rotations, flips, shifts) to the original data. This helps in increasing the size and diversity of the training data, reducing the risk of overfitting.\n","permalink":"https://www.virture.de/posts/2023/methods-to-prevent-overfitting/","summary":"Methods to prevent overfitting in Machine Laerning L2 Regularization (Ridge Regression) L2 regularization adds a penalty term to the loss function based on the squared magnitudes of the model\u0026rsquo;s weights. This penalty discourages large weight values and encourages the model to use smaller weights, leading to a smoother and more generalized solution. The regularization term is controlled by a hyperparameter (lambda or alpha) that balances the trade-off between fitting the training data and keeping the weights small.","title":"Methods to prevent overfitting"},{"content":"Major Use Case for an RNN (Recurrent Neural Network): Sequential data processing, where the order of data elements matters. RNNs are designed to handle sequences of data, such as time series data, natural language text, speech, music, and more. The main strength of RNNs lies in their ability to capture temporal dependencies and patterns in sequential data. Example Use Cases for RNNs: Natural Language Processing (NLP): RNNs are commonly used in tasks like text generation, machine translation, sentiment analysis, named entity recognition, and language modeling.\nTime Series Prediction: RNNs can be used to forecast future values in time series data, such as stock prices, weather conditions, or energy consumption.\nSpeech Recognition and Synthesis: RNNs can process audio data, making them useful in speech recognition systems, speech-to-text conversion, and text-to-speech synthesis.\nGesture Recognition: RNNs can be applied to analyze sequential data from sensors to recognize human gestures or movements.\nMusic Generation: RNNs can be employed to generate new musical sequences, such as melodies or harmonies.\nMajor Use Case for a CNN (Convolutional Neural Network): Image and visual data processing. CNNs are particularly well-suited for tasks involving grid-like data, such as images, due to their ability to capture spatial hierarchies of features. They use convolutional layers to automatically learn relevant local patterns from the input data. Example Use Cases for CNNs: Image Classification: CNNs excel at image classification tasks, where they can accurately identify objects or scenes in images.\nObject Detection: CNNs are used for object detection tasks, where they can not only classify objects but also locate them in an image using bounding boxes.\nSemantic Segmentation: CNNs can segment an image at the pixel level, assigning each pixel to a specific object or class.\nStyle Transfer: CNNs can transfer the style of one image to another, allowing for creative image editing and artistic effects.\nMedical Image Analysis: CNNs are widely used in medical imaging for tasks such as tumor detection, disease classification, and organ segmentation.\nAutonomous Vehicles: CNNs play a crucial role in self-driving cars, enabling them to interpret data from cameras and make real-time decisions.\n","permalink":"https://www.virture.de/posts/2023/rnn-cmm/","summary":"Major Use Case for an RNN (Recurrent Neural Network): Sequential data processing, where the order of data elements matters. RNNs are designed to handle sequences of data, such as time series data, natural language text, speech, music, and more. The main strength of RNNs lies in their ability to capture temporal dependencies and patterns in sequential data. Example Use Cases for RNNs: Natural Language Processing (NLP): RNNs are commonly used in tasks like text generation, machine translation, sentiment analysis, named entity recognition, and language modeling.","title":"Use Case for Recurrent Neural Network and Convolutional Neural Network"},{"content":"L1 vs. L2 Regularization: A Comparison in Machine Learning In the realm of machine learning, regularization techniques play a crucial role in controlling model complexity and preventing overfitting. Two popular regularization methods are L1 and L2 regularization, each with its distinct characteristics and impact on model weights.\nL2 Regularization L2 regularization, also known as Ridge regularization, penalizes the sum of squared weights in a model. Mathematically, it adds the square of each weight to the loss function, discouraging large weight values.\nL2 penalizes weights by their square value. The derivative of L2 is directly proportional to the weight, resulting in a gradual reduction in weight values over time. Although L2 discourages large weights, it does not drive weights to absolute zero, ensuring that all features contribute to the model.\nL1 Regularization On the other hand, L1 regularization, also called Lasso regularization, penalizes the sum of the absolute values of the weights. Unlike L2, L1 regularization has a unique characteristic related to absolute values, which makes it an efficient choice for feature selection.\nL1 penalizes weights by their absolute value, which effectively enforces sparsity in the model. The derivative of L1 is a constant (k) that remains independent of weight values, leading to sudden weight updates during optimization.\nDue to the absolute value, L1 regularization introduces a discontinuity at zero, causing certain weight updates to be zeroed out. L1 regularization is particularly useful for feature selection as it can drive some weights to exact zero, effectively excluding irrelevant features from the model.\nChoosing the Right Regularization Deciding between L1 and L2 regularization depends on the specific characteristics of the dataset and the problem at hand. L2 regularization is generally well-suited for cases where all features are expected to contribute to the model, preventing any single feature from dominating the prediction process. On the other hand, L1 regularization shines in situations where feature sparsity is desired, allowing for more interpretable models and efficient representation of wide datasets.\nIn conclusion, understanding the differences between L1 and L2 regularization is essential for effectively applying regularization techniques in machine learning models. By carefully selecting the appropriate regularization method, data scientists and machine learning practitioners can achieve better generalization and more meaningful insights from their models.\nRemember, the choice between L1 and L2 regularization is just one aspect of model regularization. In practice, it\u0026rsquo;s common to explore a combination of regularization techniques and tune their hyperparameters to achieve the best model performance for a given task.\n","permalink":"https://www.virture.de/posts/2023/l1-and-l2-regularization/","summary":"L1 vs. L2 Regularization: A Comparison in Machine Learning In the realm of machine learning, regularization techniques play a crucial role in controlling model complexity and preventing overfitting. Two popular regularization methods are L1 and L2 regularization, each with its distinct characteristics and impact on model weights.\nL2 Regularization L2 regularization, also known as Ridge regularization, penalizes the sum of squared weights in a model. Mathematically, it adds the square of each weight to the loss function, discouraging large weight values.","title":"L1 and L2 Regularization"},{"content":"Hyperparameter Tuning: Best Practices and Insights Hyperparameter tuning is a critical step in training your machine learning model, as it directly influences the model\u0026rsquo;s performance. This article discusses some key insights and practices to enhance the effectiveness of hyperparameter tuning.\nTraining Loss and its Implications Convergence of Training Loss: Ideally, the training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. This is a sign that the model is learning and adapting to the patterns in the training data. If the training loss does not converge, consider increasing the number of training epochs. Slow Decrease in Training Loss: If the training loss decreases too slowly during the training process, it might be a sign that the learning rate is too low. In such cases, consider increasing the learning rate. Be cautious, as setting the learning rate too high may prevent the training loss from converging. Variation in Training Loss: If the training loss jumps around or varies wildly, it could imply that the learning rate is too high. Decrease the learning rate to achieve a more stable decrease in training loss. Finding the Right Learning Rate and Batch Size Balancing Learning Rate, Epochs, and Batch Size: A common practice in hyperparameter tuning is to lower the learning rate while increasing the number of epochs or the batch size. This often results in better model performance. Firstly, try using large batch size values, and then decrease the batch size until you observe degradation in the model\u0026rsquo;s performance. Small Batch Sizes: Be aware that setting the batch size to a very small number can cause instability in the training process. It\u0026rsquo;s typically better to start with larger batch sizes and gradually decrease them until you observe a decrease in performance. Large Datasets: For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you\u0026rsquo;ll need to reduce the batch size to enable a batch to fit into memory. Final Thoughts Hyperparameter tuning is more of an art than a science. While these guidelines provide a good starting point, the best configuration often depends on the specifics of your dataset and model. It\u0026rsquo;s important to experiment with different combinations and tune the hyperparameters based on the feedback received from the model\u0026rsquo;s performance. Remember, patience and systematic exploration often yield the best results in hyperparameter tuning.\n","permalink":"https://www.virture.de/posts/2023/hyperparameter-tuning/","summary":"Hyperparameter Tuning: Best Practices and Insights Hyperparameter tuning is a critical step in training your machine learning model, as it directly influences the model\u0026rsquo;s performance. This article discusses some key insights and practices to enhance the effectiveness of hyperparameter tuning.\nTraining Loss and its Implications Convergence of Training Loss: Ideally, the training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.","title":"Hyperparameter Tuning"},{"content":"Types of (PII) de-identification techniques Choosing the de-identification transformation you want to use depends on the kind of data you want to de-identify and for what purpose you\u0026rsquo;re de-identifying the data. The de-identification techniques that Sensitive Data Protection supports fall into the following general categories:\nRedaction: Deletes all or part of a detected sensitive value.\nReplacement: Replaces a detected sensitive value with a specified surrogate value.\nMasking: Replaces a number of characters of a sensitive value with a specified surrogate character, such as a hash (#) or asterisk (*).\nCrypto-based tokenization: Encrypts the original sensitive data value using a cryptographic key. Sensitive Data Protection supports several types of tokenization, including transformations that can be reversed, or \u0026ldquo;re-identified.\u0026rdquo;\nBucketing: \u0026ldquo;Generalizes\u0026rdquo; a sensitive value by replacing it with a range of values. (For example, replacing a specific age with an age range, or temperatures with ranges corresponding to \u0026ldquo;Hot,\u0026rdquo; \u0026ldquo;Medium,\u0026rdquo; and \u0026ldquo;Cold.\u0026rdquo;)\nDate shifting: Shifts sensitive date values by a random amount of time.\nTime extraction: Extracts or preserves specified portions of date and time values.\n","permalink":"https://www.virture.de/posts/2023/de-identification-techniques/","summary":"Types of (PII) de-identification techniques Choosing the de-identification transformation you want to use depends on the kind of data you want to de-identify and for what purpose you\u0026rsquo;re de-identifying the data. The de-identification techniques that Sensitive Data Protection supports fall into the following general categories:\nRedaction: Deletes all or part of a detected sensitive value.\nReplacement: Replaces a detected sensitive value with a specified surrogate value.\nMasking: Replaces a number of characters of a sensitive value with a specified surrogate character, such as a hash (#) or asterisk (*).","title":"PII De-Identification techniques"},{"content":"Demystifying the Confusion Matrix: A Deep Dive into Evaluating Model Performance In the world of Machine Learning, understanding model performance is essential. One powerful tool for this purpose is the Confusion Matrix, a simple yet highly effective table layout for visualization and comprehension of your classifier\u0026rsquo;s performance.\nThe confusion matrix places the model\u0026rsquo;s predictions against the ground-truth data labels, creating an intuitive comparison grid. Each cell in this matrix represents a different aspect of the model\u0026rsquo;s performance, namely True Negatives (TN), False Negatives (FN), False Positives (FP) and True Positives (TP).\nUnderstanding the Confusion Matrix True Negatives (TN) The top-left cell in the matrix represents True Negatives. This indicates instances where the model predicted a negative result, and the actual label was indeed negative. In other words, these are the instances where the model correctly predicted a negative outcome.\nFalse Negatives (FN) The top-right cell represents False Negatives, indicating times when the model predicted a negative outcome, but the actual label was positive. This represents instances where the model should have predicted a positive outcome but did not.\nFalse Positives (FP) The bottom-left cell signifies False Positives. These are instances where the model predicted a positive outcome, but the actual label was negative. This denotes the occurrences where the model incorrectly predicted a positive outcome.\nTrue Positives (TP) Lastly, the bottom-right cell stands for True Positives, indicating times when the model correctly predicted a positive outcome.\nTo summarize, the key components of the confusion matrix are:\nTP = True positives: a positive label is correctly predicted. TN = True negatives: a negative label is correctly predicted. FP = False positives: a negative label is predicted as a positive. FN = False negatives: a positive label is predicted as a negative. Key Performance Metrics Derived from the Confusion Matrix Accuracy Accuracy is calculated as the number of correct predictions divided by the total number of predictions. It is a quick indicator of the overall performance of the model. However, accuracy may not be reliable when dealing with imbalanced datasets.\nMore generec accuracy fomularization:\naccuracy = number of correct predictions/total number of predictions Accuracy from pobability point of view:\naccuracy = (TP + TN) / number of samples Sensitivity/Recall (True Positive Rate) Sensitivity or Recall is a measure of the proportion of actual positive cases that were correctly identified by the model.\nrecall = sensitivity = TP / (TP + FN) Specificity Specificity denotes the fraction of actual negative cases that were correctly identified by the model.\nspecificity = TN / (TN + FP) Precision Precision represents the proportion of predicted positive cases that were correctly identified.\nprecision = TP / ( TP + FP) False Positive Rate (FPR) False Positive Rate denotes how often negative instances are incorrectly identified as positive.\nfalse_positive_rate = FP / ( FP + TN ) Key Note: The sum of specificity and false positive rate should always be 1!\nTrue Positive Rate (sensitivity) This metric illustrates how often \u0026ldquo;true\u0026rdquo; labels are correctly identified as \u0026ldquo;True\u0026rdquo;.\nFalse Positive Rate (False Alarm Rate) This illustrates how often \u0026ldquo;False\u0026rdquo; labels are incorrectly identified as \u0026ldquo;True\u0026rdquo;.\nWrapping Up The confusion matrix and its derived metrics provide a powerful and intuitive framework to evaluate the performance of classification models. By understanding each element and the relationships between them, we can gain valuable insights into our model\u0026rsquo;s strengths and weaknesses, and take steps to improve its performance.\n","permalink":"https://www.virture.de/posts/2023/confusion-matrix/","summary":"Demystifying the Confusion Matrix: A Deep Dive into Evaluating Model Performance In the world of Machine Learning, understanding model performance is essential. One powerful tool for this purpose is the Confusion Matrix, a simple yet highly effective table layout for visualization and comprehension of your classifier\u0026rsquo;s performance.\nThe confusion matrix places the model\u0026rsquo;s predictions against the ground-truth data labels, creating an intuitive comparison grid. Each cell in this matrix represents a different aspect of the model\u0026rsquo;s performance, namely True Negatives (TN), False Negatives (FN), False Positives (FP) and True Positives (TP).","title":"Confusion Matrix"},{"content":"Overview of Collaborative and Content-based Filtering in Recommender Systems Recommender systems have become integral to modern technology applications, spanning diverse industries from e-commerce to media streaming services. At the heart of these systems are filtering algorithms that power the delivery of personalized suggestions to each user. Two prominent types of filtering techniques are Collaborative Filtering and Content-based Filtering.\nKey Terms: Collaborative Filtering: This approach models recommendations based on a user\u0026rsquo;s past behavior, such as items previously purchased or selected, as well as numerical ratings given to those items. It also factors in the decisions made by other users who have similar patterns of behavior. The underlying premise is that users who agreed in the past will likely agree in the future. The model generated from this filtering method is then used to predict items or ratings for items that the user may find appealing.\nContent-based Filtering: In contrast, content-based filtering works by using discrete, pre-tagged characteristics of an item. For example, in a movie recommendation scenario, these characteristics could include the genre, the actors, the director, etc. This filtering approach then recommends additional items that possess similar properties to those that the user has shown a preference for in the past.\nSummary: In essence, Collaborative Filtering leverages collective user behavior to make suggestions, assuming that users with similar preferences in the past will continue to have similar preferences. Content-based Filtering, on the other hand, focuses on the properties of items themselves, suggesting items with similar characteristics to those the user has previously shown an interest in. Both approaches have their unique strengths and are employed based on the specific requirements of a recommender system.\n","permalink":"https://www.virture.de/posts/2023/collaborative-and-content-based-filtering/","summary":"Overview of Collaborative and Content-based Filtering in Recommender Systems Recommender systems have become integral to modern technology applications, spanning diverse industries from e-commerce to media streaming services. At the heart of these systems are filtering algorithms that power the delivery of personalized suggestions to each user. Two prominent types of filtering techniques are Collaborative Filtering and Content-based Filtering.\nKey Terms: Collaborative Filtering: This approach models recommendations based on a user\u0026rsquo;s past behavior, such as items previously purchased or selected, as well as numerical ratings given to those items.","title":"Collaborative and Content-based Filtering"},{"content":"Understanding Different Classification Loss Function Types in Machine Learning In machine learning, classification problems are quite common and central to many applications. The purpose of a classification problem is to predict discrete class labels, such as detecting if an email is spam or not, identifying the species of a flower based on measurements, or recognizing handwritten digits. While designing a classification model, a key part to consider is the choice of the loss function, which measures how well the model\u0026rsquo;s predictions align with the true values. The right choice of a loss function can guide your model towards better performance.\nIn this blog post, we will dive deep into various types of classification loss functions to help you make the best decision for your machine learning model.\nCategorical Hinge Loss This loss function is mainly used in Support Vector Machines (SVMs) with soft margins. Essentially, it calculates the distance between the actual and the predicted value and attempts to maximize the margin (the gap between the decision boundary and the closest data points from each class).\nTherefore, the smaller the categorical hinge loss, the larger the margin and the better your SVM performs. Binary Cross-Entropy Loss Binary cross-entropy loss, also known as log loss, is primarily used in binary classification problems, i.e., when there are only two classes to predict.\nThis loss function measures the dissimilarity between the true label and the predicted probability. One of its key features is that it heavily penalizes models that are confident about an incorrect classification. Categorical Cross-Entropy Loss Categorical cross-entropy loss is a generalization of binary cross-entropy loss and is used when there are more than two classes to predict. These classes do not necessarily have to be mutually exclusive.\nIt quantifies the distance between the actual and the predicted probability distribution. * As with binary cross-entropy, it applies a heavy penalty to confident and incorrect predictions. Sparse Categorical Cross-Entropy Loss Sparse categorical cross-entropy loss is another variant of the categorical cross-entropy loss but it is used for mutually exclusive multiclass classification problems.\nA notable advantage of this loss function is that it saves memory, making it particularly useful when dealing with large datasets. When to use which The key difference between categorical cross-entropy (cce) and sparse categorical cross-entropy (scce) lies in the format of the true and predicted class labels. Cce expects the labels to be one-hot encoded (a binary matrix representation of the class labels), which can be memory-inefficient when dealing with a large number of classes. On the other hand, scce works with integer labels, making it a memory-efficient alternative.\nCce loss function produces a one-hot array containing the probable match for each category, whereas scce loss function outputs the category index of the most likely matching category. This can lead to a significant reduction in memory usage when the number of categories is large. However, by using scce, you lose a lot of information about the probabilities of other classes, which might be important in some scenarios. In general, cce is preferred when reliability of the model is important. Nevertheless, there are situations when using scce can be beneficial:\nWhen your classes are mutually exclusive, meaning that each input only belongs to exactly one class. In this case, you don\u0026rsquo;t care at all about other close-enough predictions. When the number of categories is so large that storing the prediction output for all categories becomes infeasible or overwhelming.\nIn conclusion, selecting the right loss function is crucial for training an effective machine learning model. While this post discusses the loss functions used in classification tasks, there are many other loss functions out there suited to different types of machine learning tasks.\nAs always in machine learning, the choice of loss function should be guided by your specific problem and the nature of your data.\nSummary Classification loss functions are critical in guiding machine learning models towards optimal performance. The categorical hinge loss, predominantly used in Support Vector Machines, maximizes the margin for better model performance. Binary and categorical cross-entropy losses are used for binary and multi-class predictions respectively, heavily penalizing confident and incorrect predictions. Sparse categorical cross-entropy is suitable for mutually exclusive multi-class problems, offering memory efficiency, but sacrifices some information about other class probabilities.\nThe choice of loss function should be dictated by the specifics of your data and problem requirements.\n","permalink":"https://www.virture.de/posts/2023/classification-loss-functions/","summary":"Understanding Different Classification Loss Function Types in Machine Learning In machine learning, classification problems are quite common and central to many applications. The purpose of a classification problem is to predict discrete class labels, such as detecting if an email is spam or not, identifying the species of a flower based on measurements, or recognizing handwritten digits. While designing a classification model, a key part to consider is the choice of the loss function, which measures how well the model\u0026rsquo;s predictions align with the true values.","title":"Classification loss function types in ML"},{"content":"Azure Applied AI Services Azure Metrics Advisor\nAnalyze your business performance data and detect anomalies. Azure Cognitive Search\nEnrich data in your search indexes by using AI to analyze vision, language, and speech in content. Azure Immersive Reader\nImprove access to your web applications for new readers, language learners, and people with learning differences, such as dyslexia. Azure Bot Service\nCreate bots that can converse with your customers and partners and respond to their queries. Azure Video Analyzer\nGenerate data and business understanding from video streams. Azure Form Recognizer\nAzure Form Recognizer is an Azure service that you can use to analyze forms completed by your customers, partners, employers, or others and extract the data that they contain ","permalink":"https://www.virture.de/posts/2023/az-applied-ai/","summary":"Azure Applied AI Services Azure Metrics Advisor\nAnalyze your business performance data and detect anomalies. Azure Cognitive Search\nEnrich data in your search indexes by using AI to analyze vision, language, and speech in content. Azure Immersive Reader\nImprove access to your web applications for new readers, language learners, and people with learning differences, such as dyslexia. Azure Bot Service\nCreate bots that can converse with your customers and partners and respond to their queries.","title":"Azure AI Applied AI Services"},{"content":"Azure - AutoML Process Process Prepare data Train model Evaluate performance Deploy a predictive service Prepare Data Import data from Azure storage\nLocal files SQL databases Web files Azure Open Datasets Train model Classification (predicting categories or classes) Regression (predicting numeric values ) Time series forecasting (predicting numeric values at a future point in time) Natural language processing Computer vision Evaluate performance Cross validation RMSE (Root Mean Squared Error) NRMSE (Normalized Root Mean Squared Error) Residual History Predicted vs True chart Deploy Azure Container Instances (ACI) Azure Kubernetes Service (AKS) ","permalink":"https://www.virture.de/posts/2023/auto-ml/","summary":"Azure - AutoML Process Process Prepare data Train model Evaluate performance Deploy a predictive service Prepare Data Import data from Azure storage\nLocal files SQL databases Web files Azure Open Datasets Train model Classification (predicting categories or classes) Regression (predicting numeric values ) Time series forecasting (predicting numeric values at a future point in time) Natural language processing Computer vision Evaluate performance Cross validation RMSE (Root Mean Squared Error) NRMSE (Normalized Root Mean Squared Error) Residual History Predicted vs True chart Deploy Azure Container Instances (ACI) Azure Kubernetes Service (AKS) ","title":"Azure AutoML process"},{"content":"(YAST) A Guide to Passing the Microsoft AI-102 Certification How to lean for it..\nHello everyone,\nEmbarking on a journey to get certified can sometimes feel like a daunting endeavor. But worry not! This guide will share the most effective strategy I discovered for passing the Microsoft AI-102 Certification  most effective at least for my way of learning\u0026hellip;\nWhy This Guide? Everyone has their unique learning style. For me, its through reading and hands-on practice rather than video lectures. I find I can absorb the same amount of information much faster through reading than watching a video. Hence, I compiled this guide mainly based on text and hands-on practice resources.\nMy Learning Path Microsoft Learn provides a wealth of resources, many of which are free. Here are the steps I followed in my preparation. I did not look into other resources, but I must admit, that I already have some experience with Azure as well as with ML and AI.\nStudy Guide https://learn.microsoft.com/en-us/certifications/resources/study-guides/ai-102#skills-measured-as-of-may-2-2023\nThis is your starting point. It provides crucial information about the test, including main topics, recent news, and changes. Keep a close eye on it to stay updated. In the Study Guide you will find links to the documentation of the relevant AI/ML services. It is certainly helpful, but not absolutely necessary, to read the entire documentation. But I definitely would recommend to carefully read at least the section marked below (Overview, concepts, Rest API, Python) of all relevant services.\nMust read documentation, example from Azure AI serice\nAI 900 Exercises https://microsoftlearning.github.io/AI-900-AIFundamentals/\nThese exercises are crucial for understanding the core ML/AI services. I used them as a refresher.\nAI 102 Learning Path https://learn.microsoft.com/en-us/certifications/exams/ai-102/#two-ways-to-prepare\nThis path provides a high-level view of all the services relevant for the exam. Work through all the sub-modules diligently and carry out the contained exercises.\nExample test from MSFT Learn On this link you can schedule you final exam as well as start a test exam for fre (multiple times): https://learn.microsoft.com/en-us/certifications/azure-ai-engineer/\nFree practice examen with answers  repeat this practice tests until you scored  80 % multiple times After working through the learning path, attempt the sample test at least once. This provides an idea of where you stand knowledge-wise and points you to additional resources for weaker areas.\nAI 104 Engineer Exercises + MSLearn Cognitive Services exercises https://microsoftlearning.github.io/AI-102-AIEngineer\nThese are must-have hands-on exercises. A significant part of the exam questions is based on these exercises.\nBuild and operate ML Learning Solutions from MSFT Learn\nhttps://learn.microsoft.com/en-us/training/paths/build-ai-solutions-with-azure-ml-service/\nAlthough it didnt come up in my exam, this learning path provides a deeper understanding of MLOps under Azure.\nOpen AI Lessons (Optional) https://microsoftlearning.github.io/mslearn-openai/\nAlthough not yet part of the exam, these lessons provide useful additional practice.\nThe Exam In the exam, youll need to select upfront your peferred programming language (C# or Python). Questions about source code will then appear in that language. So you just have to learn to use the Azure AI/ML services in your preferred programming language. I went with Python..\nIts noteworthy that in my exam, about a third of the questions pertained to concrete API calls and source code. Youll need to understand how to select code parts from drop-downs or choose the syntactically correct one from a series of possible API calls.\nConclusion Microsofts learning materials proved wholly sufficient for my successful exam preparation. Of course, having a basic knowledge of Python, Machine Learning, and Azure was helpful.\nI hope this guide will prove useful for you and encourage you to delve deeper into AI/ML on Azure using MSFT Learn lessons. The possibilities are indeed abundant.\nBest of luck with your exam preparations and remember, the journey is just as important as the destination!\n","permalink":"https://www.virture.de/posts/2023/ai-102-cert-leraning-path/","summary":"(YAST) A Guide to Passing the Microsoft AI-102 Certification How to lean for it..\nHello everyone,\nEmbarking on a journey to get certified can sometimes feel like a daunting endeavor. But worry not! This guide will share the most effective strategy I discovered for passing the Microsoft AI-102 Certification  most effective at least for my way of learning\u0026hellip;\nWhy This Guide? Everyone has their unique learning style. For me, its through reading and hands-on practice rather than video lectures.","title":"A guide to passing the Microsoft AI-102 Certification"},{"content":"Quickly reset VSCode to factory settings Introduction A handy guide on how to swiftly reset Visual Studio Code (VS Code) to its factory settings. Whether you\u0026rsquo;re experiencing performance issues, conflicts with extensions, or simply want a fresh start, resetting VS Code can often be the solution.\nWe\u0026rsquo;ll focus on resetting VS Code on macOS, where we\u0026rsquo;ll walk you through a series of commands to remove the necessary files and directories associated with VS Code. This includes removing the \u0026ldquo;.vscode\u0026rdquo; directory, clearing the application support, caches, and saved application state folders.\nBy following the steps outlined in this guide, you\u0026rsquo;ll be able to reset your VS Code installation and start with a clean slate. This can help resolve various issues and ensure a smooth and optimized coding experience.\nSo, if you\u0026rsquo;re ready to give your VS Code a fresh start and restore it to its default configuration, let\u0026rsquo;s dive in and discover how to quickly reset VS Code to factory settings!\nHere we go For macOS $ rm -rf \u0026#34;$HOME/.vscode\u0026#34; $ rm -rf \u0026#34;$HOME/Library/Application Support/Code\u0026#34; $ rm -rf \u0026#34;$HOME/Library/Caches/com.microsoft.VSCode\u0026#34; $ rm -rf \u0026#34;$HOME/Library/Saved Application State/com.microsoft.VSCode.savedState\u0026#34; found at: https://www.shellhacks.com/reset-visual-studio-code/\n","permalink":"https://www.virture.de/posts/2023/reset-vscode/","summary":"Quickly reset VSCode to factory settings Introduction A handy guide on how to swiftly reset Visual Studio Code (VS Code) to its factory settings. Whether you\u0026rsquo;re experiencing performance issues, conflicts with extensions, or simply want a fresh start, resetting VS Code can often be the solution.\nWe\u0026rsquo;ll focus on resetting VS Code on macOS, where we\u0026rsquo;ll walk you through a series of commands to remove the necessary files and directories associated with VS Code.","title":"Reset VSCode"},{"content":"CLI Login to azure for Terraform deployments Introduction In this article, we\u0026rsquo;ll explore the process of logging in to Azure for Terraform deployments and managing remote state. When working with Terraform to provision infrastructure in Azure, it\u0026rsquo;s crucial to establish a secure and authenticated connection to your Azure account.\nWe\u0026rsquo;ll start by discussing the steps to log in to Azure using the az login command. This command will prompt you to authenticate with your Azure credentials, allowing you to access and manage your Azure resources within your Terraform configuration.\nNext, we\u0026rsquo;ll delve into managing remote state in Terraform. If you have a tfstate file in a different subscription, resource group, or storage account, we\u0026rsquo;ll guide you through the process of exporting the necessary environment variable, ARM_ACCESS_KEY. This variable ensures that Terraform can authenticate and access the remote state file properly.\nBy following the instructions outlined in this guide, you\u0026rsquo;ll be able to log in to Azure for your Terraform deployments and seamlessly manage remote state, even if your tfstate file resides in a different subscription.\nRemark This approach should only be used for local development e.g via vscode and should not be used for CI/CD pipelines due to security risks\nHow to login to azure az login Set correct subscription az account set --subscription $SUBSCRIPTION_NAME If you have a tfstate in different subscription If you have an tfstate file in different subscriptio, resource group, or storage account you have to export the following environment variable:\nexport ARM_ACCESS_KEY=$(az storage account keys list --resource-group $RESGROUP --account-name $STORAGE_ACCOUNT_NAME --subscription $SUBSCRIPTION --query \u0026#39;[0].value\u0026#39; -o tsv) After this terraform init shoud work properly.\n","permalink":"https://www.virture.de/posts/2023/terraform-azure-login/","summary":"CLI Login to azure for Terraform deployments Introduction In this article, we\u0026rsquo;ll explore the process of logging in to Azure for Terraform deployments and managing remote state. When working with Terraform to provision infrastructure in Azure, it\u0026rsquo;s crucial to establish a secure and authenticated connection to your Azure account.\nWe\u0026rsquo;ll start by discussing the steps to log in to Azure using the az login command. This command will prompt you to authenticate with your Azure credentials, allowing you to access and manage your Azure resources within your Terraform configuration.","title":"Terraform Azure - Login and remote state"},{"content":"Linux Special characters in a shell (@, , , ~) Introduction Let\u0026rsquo;s explore the usage of special characters in a shell environment, specifically focusing on Linux and macOS systems. These special characters, such as \u0026ldquo;@\u0026rdquo;, \u0026ldquo;\u0026rdquo;, \u0026ldquo;\u0026rdquo;, \u0026ldquo;~\u0026rdquo;, and \u0026ldquo;|\u0026rdquo;, can be particularly useful when working with commands and scripts.\nHere we go right alt + Q = @ right alt +  = right alt + ^ = | right alt + e =  right alt + n = ~\nalternatively:\nctrl + left alt + q = @ ctrl + left alt +  = ctrl + left alt + ^ = | ctrl + left alt + e =  ctrl + left alt + n = ~~\nwith a mac keyboard\nopt + n = ~ opt + l = @ opt + 7 = | opt + shift + 7 = \\\nshow hidden files on macos:\nshift + command + .\nHave fun with different keyboard settings on different platforms especially when \u0026lsquo;flavored\u0026rsquo; with different language configurations - you\u0026rsquo;ll start loving complex passwords\u0026hellip;\n","permalink":"https://www.virture.de/posts/2023/linux-shell-special-chars/","summary":"Linux Special characters in a shell (@, , , ~) Introduction Let\u0026rsquo;s explore the usage of special characters in a shell environment, specifically focusing on Linux and macOS systems. These special characters, such as \u0026ldquo;@\u0026rdquo;, \u0026ldquo;\u0026rdquo;, \u0026ldquo;\u0026rdquo;, \u0026ldquo;~\u0026rdquo;, and \u0026ldquo;|\u0026rdquo;, can be particularly useful when working with commands and scripts.\nHere we go right alt + Q = @ right alt +  = right alt + ^ = | right alt + e =  right alt + n = ~","title":"Special Chars in a Linux Shell"},{"content":"Setup an python environment in vscode Introduction In this article, we\u0026rsquo;ll explore a step-by-step guide on how to swiftly set up a Python environment in Visual Studio Code (VS Code). Whether you\u0026rsquo;re a Python beginner or a seasoned developer, having an efficient and organized development environment is crucial for productive coding sessions.\nWe\u0026rsquo;ll start by creating a new environment using the Python virtual environment module. By isolating our project dependencies, we ensure a clean and controlled environment. We\u0026rsquo;ll cover the necessary commands to create a new environment directory and activate it.\nOnce our environment is activated, we\u0026rsquo;ll dive into managing package installations. You\u0026rsquo;ll learn how to list the installed packages using pip and even install specific versions of libraries like pandas. Additionally, we\u0026rsquo;ll reveal a handy trick to generate a requirements.txt file on the fly, simplifying the process of documenting project dependencies.\nBy the end of this guide, you\u0026rsquo;ll have a streamlined Python environment set up in VS Code, ready to tackle your coding challenges efficiently. So, let\u0026rsquo;s get started and empower your Python development workflow in no time!\nLet\u0026rsquo;s dig into it: Setup an python environment in vscode In shell:\nCreating new environment python3 -m venv \u0026lt;new-dir\u0026gt; Activating environment source \u0026lt;new-dir\u0026gt;/bin/activate List installed packages pip3 list Deactivate environment with: deactivate Install a specific version of a lib: pip3 install pandas==1.2.5 Create requirements.txt on the fly There are more fancy aproaches, but this one is very quick:\npip freeze \u0026gt;\u0026gt; requirements.txt ","permalink":"https://www.virture.de/posts/2023/python-environment/","summary":"Setup an python environment in vscode Introduction In this article, we\u0026rsquo;ll explore a step-by-step guide on how to swiftly set up a Python environment in Visual Studio Code (VS Code). Whether you\u0026rsquo;re a Python beginner or a seasoned developer, having an efficient and organized development environment is crucial for productive coding sessions.\nWe\u0026rsquo;ll start by creating a new environment using the Python virtual environment module. By isolating our project dependencies, we ensure a clean and controlled environment.","title":"Python environment"},{"content":"How to get own external IP via terminal (curl) This command even provides some additional interesting information:\ncurl ipinfo.io If you prefer the \u0026ldquo;Way of the Browser\u0026rdquo;: https://whatismyipaddress.com/\n","permalink":"https://www.virture.de/posts/2023/get-external-ip/","summary":"How to get own external IP via terminal (curl) This command even provides some additional interesting information:\ncurl ipinfo.io If you prefer the \u0026ldquo;Way of the Browser\u0026rdquo;: https://whatismyipaddress.com/","title":"Getting your own external IP via curl"},{"content":"Publish a website or a blog on github pages Introduction: Are you looking to share your website with the world? GitHub Pages provides an easy and free way to publish your static website directly from your GitHub repository. In this blog post, we\u0026rsquo;ll walk you through the process of publishing your website on GitHub Pages, allowing you to showcase your work and make it accessible to a global audience.\nStep 1: Create a Repository: Start by creating a new repository on GitHub. Ensure that you name it in the following format: .github.io. This naming convention is essential for GitHub Pages to recognize it as your personal website.\nStep 2: Add Your Website Files: Once your repository is created, add your website files to the repository. Ensure that your website is a static site, as GitHub Pages only supports hosting static content. You can include HTML, CSS, JavaScript, and any other necessary assets.\nStep 3: Enable GitHub Pages: Now, navigate to the repository\u0026rsquo;s settings page. Scroll down to the \u0026ldquo;GitHub Pages\u0026rdquo; section. Under the \u0026ldquo;Source\u0026rdquo; heading, select the branch that contains your website files. Typically, it will be the \u0026ldquo;main\u0026rdquo; branch. Select the folder in which your root is placed, e.g. index.html. It should lokke like this (in this example the subflder \u0026lsquo;docs\u0026rsquo; contains the websites root)\nStep 4: Publish Your Website: After enabling GitHub Pages and selecting the source branch, click on the provided link to access your published website. It might take a few moments for your changes to propagate. Congratulations! Your website is now live on GitHub Pages.\nStep 6: Update Your Website: As you make updates to your website, commit and push the changes to your repository. GitHub Pages will automatically reflect those changes, allowing you to iterate and improve your site seamlessly.\nConclusion: Publishing your website on GitHub Pages provides a simple and convenient way to share your work with the world. By following the steps outlined in this guide, you can effortlessly showcase your static website to a global audience.\n","permalink":"https://www.virture.de/posts/2023/publish-website-github/","summary":"Publish a website or a blog on github pages Introduction: Are you looking to share your website with the world? GitHub Pages provides an easy and free way to publish your static website directly from your GitHub repository. In this blog post, we\u0026rsquo;ll walk you through the process of publishing your website on GitHub Pages, allowing you to showcase your work and make it accessible to a global audience.\nStep 1: Create a Repository: Start by creating a new repository on GitHub.","title":"Publish a website on github pages"},{"content":"How to enable azure cli autocomplete in macos zsh shell 1. Install azure cli brew update \u0026amp;\u0026amp; brew install azure-cli All dependencies will be instaled automatically.\nThe Homebrew formula of Azure CLI installs a completion file named az in the Homebrew-managed completions directory (default location is /usr/local/etc/bash_completion.d/). To enable completion, follow Homebrew\u0026rsquo;s instructions here: https://docs.brew.sh/Shell-Completion.\nFor Zsh, add the following two lines to the bottom of your .zshrc file, then save and reload your Zsh profile:\nautoload bashcompinit \u0026amp;\u0026amp; bashcompinit source $(brew --prefix)/etc/bash_completion.d/az That\u0026rsquo;s it!\n","permalink":"https://www.virture.de/posts/2023/azure-cli-autocomplete/","summary":"How to enable azure cli autocomplete in macos zsh shell 1. Install azure cli brew update \u0026amp;\u0026amp; brew install azure-cli All dependencies will be instaled automatically.\nThe Homebrew formula of Azure CLI installs a completion file named az in the Homebrew-managed completions directory (default location is /usr/local/etc/bash_completion.d/). To enable completion, follow Homebrew\u0026rsquo;s instructions here: https://docs.brew.sh/Shell-Completion.\nFor Zsh, add the following two lines to the bottom of your .zshrc file, then save and reload your Zsh profile:","title":"Azure CLI - activate azure cli autocomplete in zsh"}]